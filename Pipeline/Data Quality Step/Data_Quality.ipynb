{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lQ4wLrKzbxR"
      },
      "source": [
        "# Data Quality - Kaggle Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ9SsFpQ1ciY"
      },
      "source": [
        "## Objectives of the Notebook\n",
        "The purpose of this notebook is to apply a standardized cleaning and validating data quality for datasets obtained from the Kaggle platform. This is a dataset only for experimentation. Our goal is to create a pipeline that would fit any Kaggle dataset. While reviewing different projects and their associated datasets, this one was chosen manually after a search using the Kaggle API, where the best datasets were filtered based on user votes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqXoxdBu0CyD"
      },
      "source": [
        "## Data Quality (DQ)\n",
        "Refers to the degree to which data is suitable for its intended use. In other words, data must be accurate, complete, relevant, and reliable in the context in which it is used. Poor data quality can seriously affect the results of analyses, predictive models, and business decisions.\n",
        "\n",
        "This notebook implements a cleaning and validation pipeline based on the framework proposed in the paper **“The Five Facets of Data Quality Assessment,”** which organizes data quality assessment through:\n",
        "\n",
        "5 Key Dimensions of DQ:\n",
        "\n",
        "- Accuracy: Does the data reflect reality?\n",
        "- Representativeness: Is it representative of the domain it models?\n",
        "- Completeness: Is data missing or well represented?\n",
        "- Relevance: Is it useful for the stated objective?\n",
        "\n",
        "5 Facets for evaluation:\n",
        "\n",
        "- Data: analysis of digital data and its metadata.\n",
        "- Source: traceability and quality of the dataset's origin.\n",
        "- System: technical support, reproducibility, and compliance.\n",
        "- Task: relevance and adaptation to the specific problem.\n",
        "- Human: user perception, intention, and needs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah7makMHg43L"
      },
      "source": [
        "## Applying the Five Facets to the Analysis of Kaggle Datasets\n",
        "\n",
        "After reviewing the approach proposed in the paper \"The Five Facets of Data Quality Assessment\" and comparing it with the types of datasets available on Kaggle, We following this adaptation for our case, focused on building a standardized data quality analysis framework:\n",
        "\n",
        "1. **Data Facet:**\n",
        "This is the most relevant facet for our use case. We can analyze structure, data types, missing values, schema consistency, and placeholder values. The most challenging part is evaluating accuracy against reference data, since Kaggle datasets typically don't come with ground-truth benchmarks. However, all other aspects can be assessed in an automated and scalable way.\n",
        "\n",
        "2. **Source Facet:**\n",
        "We can extract the dataset author's name and check if a source is provided in the dataset description. However, we cannot access version history or transformation lineage through the Kaggle API. Traceability is limited to what is manually documented.\n",
        "\n",
        "3. **System Facet:**\n",
        "Kaggle maintains the original state of the dataset, allowing us to assume reproducibility. While we don't have access to low-level storage or file system metadata, this doesn't significantly affect our analysis.\n",
        "\n",
        "4. **Task Facet:**\n",
        "If the dataset supports multiple tasks or lacks a clearly defined purpose, this facet becomes harder to apply automatically.\n",
        "\n",
        "5. **Human Facet:**\n",
        "This facet depends on knowing the end users, their domain expertise, and the usage context, information we can't access automatically. This would require manual input and should be treated as part of the metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SxirnYgyoYy"
      },
      "source": [
        "## Data Quality Metric Thresholds\n",
        "\n",
        "There is currently no universally accepted standard for assessing data quality using metrics such as missing values, duplicates, or cardinality. The literature consistently emphasizes that data quality thresholds must be defined based on the specific context, purpose, and domain of use. Papers such as Measuring Data Quality in Information Systems Research, The Challenges of Data Quality and Data Quality Assessment in the Big Data Era, and Increasing Trust in Real-World Evidence Through Evaluation of Observational Data Quality support this view, arguing that rigid, one-size-fits-all thresholds are inadequate. Instead, they advocate for flexible, context-driven frameworks and transparent reporting to ensure that data quality assessments are meaningful and aligned with the goals of the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set Project and Connect to the Kaggle API and Download the File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import math\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your Kaggle credentials:\n",
        "os.environ['KAGGLE_USERNAME'] = \"abnerasteteh\"\n",
        "os.environ['KAGGLE_KEY'] = \"5dec29fba975d4119a37855b3653d27d\"\n",
        "\n",
        "# Authentication\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Configuration\n",
        "WEIGHTS = {\n",
        "    \"completeness\": 0.45,\n",
        "    \"uniqueness\": 0.25,\n",
        "    \"outliers\": 0.30\n",
        "}\n",
        "ALERT_THRESHOLD = 75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of Kaggle projects\n",
        "kernel_refs = [\n",
        "    #\"joelknapp/student-performance-analysis/\",\n",
        "    \"aremoto/retail-sales-forecast/\",\n",
        "]\n",
        "\n",
        "# Creation of the base directory and complementary functions\n",
        "base_dir = \"kaggle_notebooks\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "def safe_name(ref):\n",
        "    return re.sub(r'[^\\w\\-]', '_', ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_for_json(obj):\n",
        "    \"\"\"Convert Python objects to JSON serializable format\"\"\"\n",
        "    if isinstance(obj, np.bool_):\n",
        "        return bool(obj)\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, (pd.Timestamp, datetime)):\n",
        "        return obj.isoformat()\n",
        "    elif hasattr(obj, 'item'):\n",
        "        return obj.item()\n",
        "    elif hasattr(obj, 'tolist'):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_for_json(item) for item in obj]\n",
        "    return obj\n",
        "\n",
        "def safe_json_dump(data, file_path, **kwargs):\n",
        "    \"\"\"Safely dump data to JSON with proper type conversion\"\"\"\n",
        "    converted_data = convert_for_json(data)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(converted_data, f, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Facet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_csv_metadata(file_path):\n",
        "   \"\"\"Extract comprehensive metadata and quality metrics from a CSV file.\"\"\"\n",
        "   file_path = Path(file_path)\n",
        "   metadata = {\n",
        "       'filename': file_path.name,\n",
        "       'file_path': str(file_path),\n",
        "       'analysis_timestamp': datetime.now().isoformat(),\n",
        "       'file_exists': file_path.exists(),\n",
        "       'file_info': {},\n",
        "       'csv_schema': {},\n",
        "       'data_quality': {},\n",
        "       'errors': []\n",
        "   }\n",
        "\n",
        "   if not file_path.exists():\n",
        "       metadata['errors'].append(f\"File not found: {file_path}\")\n",
        "       return metadata\n",
        "\n",
        "   try:\n",
        "       # Extract file system metadata\n",
        "       stat_info = file_path.stat()\n",
        "       metadata['file_info'] = {\n",
        "           'size_bytes': stat_info.st_size,\n",
        "           'size_mb': round(stat_info.st_size / (1024*1024), 2),\n",
        "           'creation_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),\n",
        "           'modification_time': datetime.fromtimestamp(stat_info.st_mtime).isoformat(),\n",
        "           'extension': file_path.suffix.lower(),\n",
        "       }\n",
        "\n",
        "       try:\n",
        "           # Read and analyze CSV data\n",
        "           df_full = pd.read_csv(file_path)\n",
        "\n",
        "           # Extract schema information\n",
        "           metadata['csv_schema'] = {\n",
        "               'total_rows': len(df_full),\n",
        "               'total_columns': len(df_full.columns),\n",
        "               'columns': list(df_full.columns),\n",
        "               'column_types': df_full.dtypes.astype(str).to_dict(),\n",
        "               'memory_usage_mb': round(df_full.memory_usage(deep=True).sum() / (1024*1024), 2),\n",
        "               'shape': df_full.shape\n",
        "           }\n",
        "\n",
        "           # Calculate data quality metrics\n",
        "           metadata['data_quality'] = {\n",
        "               'null_values_per_column': df_full.isnull().sum().to_dict(),\n",
        "               'null_percentage_per_column': (df_full.isnull().sum() / len(df_full) * 100).round(2).to_dict(),\n",
        "               'total_null_values': int(df_full.isnull().sum().sum()),\n",
        "               'duplicate_rows': int(df_full.duplicated().sum()),\n",
        "               'unique_values_per_column': df_full.nunique().to_dict(),\n",
        "               'completeness_score': round((1 - df_full.isnull().sum().sum() / df_full.size) * 100, 2)\n",
        "           }\n",
        "\n",
        "           # Extract numeric statistics if numeric columns exist\n",
        "           numeric_columns = df_full.select_dtypes(include=['number']).columns\n",
        "           if len(numeric_columns) > 0:\n",
        "               numeric_stats = df_full[numeric_columns].describe().to_dict()\n",
        "               metadata['numeric_statistics'] = convert_for_json(numeric_stats)\n",
        "\n",
        "       except Exception as e:\n",
        "           metadata['errors'].append(f\"Error reading CSV: {str(e)}\")\n",
        "\n",
        "   except Exception as e:\n",
        "       metadata['errors'].append(f\"Error accessing file: {str(e)}\")\n",
        "\n",
        "   return metadata\n",
        "\n",
        "def analyze_multiple_csvs(directory_path):\n",
        "   \"\"\"Analyze all CSV files in a given directory and generate a comprehensive report.\"\"\"\n",
        "   directory_path = Path(directory_path)\n",
        "   analysis_report = {\n",
        "       'directory': str(directory_path),\n",
        "       'analysis_timestamp': datetime.now().isoformat(),\n",
        "       'csv_files_found': 0,\n",
        "       'files_analysis': {},\n",
        "       'summary': {'errors': []}\n",
        "   }\n",
        "\n",
        "   if not directory_path.exists():\n",
        "       analysis_report['summary']['errors'].append(f\"Directory does not exist: {directory_path}\")\n",
        "       return analysis_report\n",
        "\n",
        "   # Find and analyze all CSV files\n",
        "   csv_files = list(directory_path.glob('*.csv'))\n",
        "   analysis_report['csv_files_found'] = len(csv_files)\n",
        "\n",
        "   for csv_file in csv_files:\n",
        "       print(f\"Analyzing: {csv_file.name}\")\n",
        "       file_metadata = extract_csv_metadata(csv_file)\n",
        "       analysis_report['files_analysis'][csv_file.name] = file_metadata\n",
        "\n",
        "   return analysis_report\n",
        "\n",
        "def calculate_completeness(null_percentage_per_column):\n",
        "   \"\"\"Calculate data completeness score based on null value percentages.\"\"\"\n",
        "   return 100 - np.mean(list(null_percentage_per_column.values()))\n",
        "\n",
        "def calculate_uniqueness(total_rows, duplicate_rows):\n",
        "   \"\"\"Calculate data uniqueness score based on duplicate row count.\"\"\"\n",
        "   return max(0, 100 - (duplicate_rows / total_rows * 100))\n",
        "\n",
        "def calculate_outliers(data):\n",
        "   \"\"\"Calculate outlier penalty score using 3-sigma rule for numeric columns.\"\"\"\n",
        "   penalties = []\n",
        "   for col, stats in data.get(\"numeric_statistics\", {}).items():\n",
        "       mean = stats.get(\"mean\", 0)\n",
        "       std = stats.get(\"std\", 0)\n",
        "       col_min = stats.get(\"min\", mean)\n",
        "       col_max = stats.get(\"max\", mean)\n",
        "       # Apply 3-sigma rule for outlier detection\n",
        "       if std > 0:\n",
        "           if col_min < mean - 3 * std or col_max > mean + 3 * std:\n",
        "               penalties.append(1)\n",
        "           else:\n",
        "               penalties.append(0)\n",
        "   return 100 - (np.mean(penalties) * 100) if penalties else 100\n",
        "\n",
        "def calculate_global_score(dataset):\n",
        "   \"\"\"Calculate overall data quality score using weighted metrics.\"\"\"\n",
        "   completeness = calculate_completeness(dataset[\"data_quality\"][\"null_percentage_per_column\"])\n",
        "   uniqueness = calculate_uniqueness(dataset[\"csv_schema\"][\"total_rows\"], dataset[\"data_quality\"][\"duplicate_rows\"])\n",
        "   outliers = calculate_outliers(dataset)\n",
        "\n",
        "   # Calculate weighted global score\n",
        "   global_score = (\n",
        "       completeness * WEIGHTS[\"completeness\"] +\n",
        "       uniqueness * WEIGHTS[\"uniqueness\"] +\n",
        "       outliers * WEIGHTS[\"outliers\"]\n",
        "   )\n",
        "\n",
        "   return {\n",
        "       \"completeness\": completeness,\n",
        "       \"uniqueness\": uniqueness,\n",
        "       \"outliers\": outliers,\n",
        "       \"global_score\": global_score\n",
        "   }\n",
        "\n",
        "def clean_dataset(file_path, output_dir, lower_quantile=0.05, upper_quantile=0.95, max_missing_frac=0.1):\n",
        "   \"\"\"Clean dataset by handling missing values, duplicates, and outliers.\"\"\"\n",
        "   df = pd.read_csv(file_path)\n",
        "   numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "   # Handle missing values in numeric columns using median imputation\n",
        "   if len(numeric_cols) > 0:\n",
        "       imputer = SimpleImputer(strategy='median')\n",
        "       df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "   # Handle missing values in categorical columns\n",
        "   categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "   for col in categorical_cols:\n",
        "       missing_frac = df[col].isna().mean()\n",
        "       if 0 < missing_frac <= max_missing_frac:\n",
        "           df = df[df[col].notna()]\n",
        "\n",
        "   # Remove duplicate rows\n",
        "   df = df.drop_duplicates()\n",
        "\n",
        "   # Handle outliers using quantile-based clipping\n",
        "   for col in numeric_cols:\n",
        "       lower = df[col].quantile(lower_quantile)\n",
        "       upper = df[col].quantile(upper_quantile)\n",
        "       df[col] = np.clip(df[col], lower, upper)\n",
        "\n",
        "   # Save cleaned dataset\n",
        "   output_dir.mkdir(exist_ok=True)\n",
        "   output_path = output_dir / f\"clean_{Path(file_path).name}\"\n",
        "   df.to_csv(output_path, index=False)\n",
        "   print(f\"Cleaned dataset saved at {output_path}\")\n",
        "   return output_path\n",
        "\n",
        "def plot_histograms(file_data, save_dir=None):\n",
        "    \"\"\"Generate histogram plots for all numeric columns in a dataset with statistical annotations.\"\"\"\n",
        "    file_path = file_data.get('file_path')\n",
        "    filename = file_data.get('filename', 'File')\n",
        "\n",
        "    if not file_path or not Path(file_path).exists():\n",
        "        print(f\"Cannot open {filename} for plotting\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df_full = pd.read_csv(file_path)\n",
        "        \n",
        "        # Select plotting style\n",
        "        available_styles = plt.style.available\n",
        "        preferred_styles = ['seaborn', 'ggplot', 'fivethirtyeight', 'bmh']\n",
        "        selected_style = 'classic'\n",
        "\n",
        "        for style in preferred_styles:\n",
        "            if style in available_styles:\n",
        "                selected_style = style\n",
        "                break\n",
        "\n",
        "        plt.style.use(selected_style)\n",
        "\n",
        "        # Get numeric columns for plotting\n",
        "        numeric_columns = df_full.select_dtypes(include=['number']).columns\n",
        "        n = len(numeric_columns)\n",
        "        if n == 0:\n",
        "            print(\"No numeric columns to plot\")\n",
        "            return\n",
        "\n",
        "        # Set up subplot grid\n",
        "        colors = plt.cm.tab10.colors\n",
        "        cols = min(4, n)\n",
        "        rows = math.ceil(n / cols)\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4.5))\n",
        "        axes = axes.flatten() if n > 1 else [axes]\n",
        "\n",
        "        # Create histogram for each numeric column\n",
        "        for i, col in enumerate(numeric_columns):\n",
        "            col_data = df_full[col].dropna()\n",
        "            unique_vals = col_data.unique()\n",
        "            num_unique = len(unique_vals)\n",
        "            bins = num_unique if num_unique <= 15 else 'auto'\n",
        "\n",
        "            # Calculate statistical measures\n",
        "            mean_val = col_data.mean()\n",
        "            median_val = col_data.median()\n",
        "            std_val = col_data.std()\n",
        "            skewness = col_data.skew()\n",
        "            kurt = col_data.kurtosis()\n",
        "\n",
        "            # Plot histogram\n",
        "            axes[i].hist(col_data, bins=bins,\n",
        "                        edgecolor='white',\n",
        "                        linewidth=1.2,\n",
        "                        color=colors[i % len(colors)],\n",
        "                        alpha=0.8)\n",
        "\n",
        "            # Add statistical lines\n",
        "            axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:.2f}')\n",
        "            axes[i].axvline(median_val, color='green', linestyle='--', linewidth=1.5, label=f'Median: {median_val:.2f}')\n",
        "\n",
        "            # Customize plot appearance\n",
        "            axes[i].set_title(f\"Distribution of {col}\", pad=15, fontsize=12, fontweight='bold')\n",
        "            axes[i].set_xlabel(col, labelpad=10)\n",
        "            axes[i].set_ylabel('Frequency', labelpad=10)\n",
        "            axes[i].grid(axis='y', alpha=0.3)\n",
        "\n",
        "            # Add statistics text box\n",
        "            stats_text = (f\"Mean: {mean_val:.2f}\\n\"\n",
        "                        f\"Median: {median_val:.2f}\\n\"\n",
        "                        f\"Std. Dev.: {std_val:.2f}\\n\"\n",
        "                        f\"Skewness: {skewness:.2f}\\n\"\n",
        "                        f\"Kurtosis: {kurt:.2f}\")\n",
        "\n",
        "            axes[i].text(0.95, 0.95, stats_text,\n",
        "                        transform=axes[i].transAxes,\n",
        "                        ha='right', va='top',\n",
        "                        bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n",
        "\n",
        "            # Adjust x-axis ticks for discrete variables\n",
        "            if num_unique <= 15:\n",
        "                axes[i].set_xticks(np.linspace(col_data.min(), col_data.max(), num=min(10, num_unique)))\n",
        "\n",
        "            axes[i].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "        # Remove empty subplots\n",
        "        for j in range(i+1, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        # Add main title and adjust layout\n",
        "        fig.suptitle(f\"Distribution Analysis - {filename}\\n(Style: {selected_style})\",\n",
        "                    y=1.02, fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot if directory specified\n",
        "        if save_dir:\n",
        "            save_path = save_dir / f\"histogram_{Path(filename).stem}.png\"\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"Histogram saved: {save_path}\")\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting {filename}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Source Facet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataset_evaluation(dataset_name):\n",
        "   \"\"\"Evaluate a Kaggle dataset's reliability and gather comprehensive metadata.\"\"\"\n",
        "   owner_slug, dataset_slug = dataset_name.split('/')\n",
        "   ref = f\"{owner_slug}/{dataset_slug}\"\n",
        "\n",
        "   # 1. Basic dataset information\n",
        "   results = api.dataset_list(search=dataset_slug, user=owner_slug)\n",
        "   dataset_info = None\n",
        "\n",
        "   for ds in results:\n",
        "       if ds.ref == ref:\n",
        "           dataset_info = {\n",
        "               \"dataset_name\": ds.title,\n",
        "               \"subtitle\": getattr(ds, \"subtitle\", \"\"),\n",
        "               \"description\": getattr(ds, \"description\", \"\"),\n",
        "               \"license\": getattr(ds, \"license_name\", \"Unknown\"),\n",
        "               \"author\": ds.creator_name,\n",
        "               \"kaggle_id\": ref,\n",
        "               \"kaggle_url\": f\"https://www.kaggle.com/datasets/{ref}\",\n",
        "               \"total_downloads\": ds.download_count,\n",
        "               \"votes\": getattr(ds, 'vote_count', 0),\n",
        "               \"is_private\": ds.is_private,\n",
        "               \"is_featured\": ds.is_featured,\n",
        "               \"usability_rating\": getattr(ds, \"usabilityRating\", None),\n",
        "               \"download_date\": datetime.now().date().isoformat(),\n",
        "               \"creation_date\": getattr(ds, \"creationDate\", \"Not available\"),\n",
        "               \"last_updated\": getattr(ds, \"lastUpdated\", \"Not available\")\n",
        "           }\n",
        "           break\n",
        "\n",
        "   if not dataset_info:\n",
        "       raise ValueError(f\"Dataset {ref} not found\")\n",
        "\n",
        "   # 2. Author information (reputation and activity)\n",
        "   def get_all_datasets(user):\n",
        "       \"\"\"Retrieve all datasets published by a user across multiple pages.\"\"\"\n",
        "       page = 1\n",
        "       all_datasets = []\n",
        "       while True:\n",
        "           try:\n",
        "               datasets = api.dataset_list(user=user, page=page)\n",
        "               if not datasets:\n",
        "                   break\n",
        "               all_datasets.extend(datasets)\n",
        "               page += 1\n",
        "           except:\n",
        "               break\n",
        "       return all_datasets\n",
        "\n",
        "   def get_all_kernels(user):\n",
        "       \"\"\"Retrieve all kernels/notebooks published by a user across multiple pages.\"\"\"\n",
        "       page = 1\n",
        "       all_kernels = []\n",
        "       while True:\n",
        "           try:\n",
        "               kernels = api.kernels_list(user=user, page=page)\n",
        "               if not kernels:\n",
        "                   break\n",
        "               all_kernels.extend(kernels)\n",
        "               page += 1\n",
        "           except:\n",
        "               break\n",
        "       return all_kernels\n",
        "\n",
        "   def get_user_followers(username):\n",
        "       \"\"\"Get user's follower count, following count, and tier information.\"\"\"\n",
        "       try:\n",
        "           user_info = api.user_read(username)\n",
        "           followers = getattr(user_info, \"followerCount\", \"Not available\")\n",
        "           following = getattr(user_info, \"followingCount\", \"Not available\")\n",
        "           tier = getattr(user_info, \"tier\", \"Not available\")\n",
        "           return followers, following, tier\n",
        "       except:\n",
        "           return \"Not available\", \"Not available\", \"Not available\"\n",
        "\n",
        "   try:\n",
        "       # Gather comprehensive author statistics\n",
        "       author_datasets = get_all_datasets(owner_slug)\n",
        "       total_datasets = len(author_datasets)\n",
        "       author_notebooks = get_all_kernels(owner_slug)\n",
        "       total_notebooks = len(author_notebooks)\n",
        "       followers, following, tier = get_user_followers(owner_slug)\n",
        "\n",
        "       # Calculate aggregate metrics\n",
        "       total_downloads = sum(getattr(ds, 'download_count', 0) for ds in author_datasets)\n",
        "       total_votes = sum(getattr(ds, 'vote_count', 0) for ds in author_datasets)\n",
        "       notebook_votes = sum(getattr(nb, 'voteCount', 0) for nb in author_notebooks)\n",
        "\n",
        "       author_stats = {\n",
        "           \"total_datasets\": total_datasets,\n",
        "           \"total_notebooks\": total_notebooks,\n",
        "           \"total_dataset_downloads\": total_downloads,\n",
        "           \"total_dataset_votes\": total_votes,\n",
        "           \"total_notebook_votes\": notebook_votes,\n",
        "           \"follower_count\": followers,\n",
        "           \"following_count\": following,\n",
        "           \"author_tier\": tier,\n",
        "           \"avg_downloads_per_dataset\": round(total_downloads / total_datasets, 2) if total_datasets > 0 else 0,\n",
        "           \"avg_votes_per_dataset\": round(total_votes / total_datasets, 2) if total_datasets > 0 else 0\n",
        "       }\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Could not retrieve author statistics: {e}\")\n",
        "       author_stats = {\"error\": \"Author information not available\"}\n",
        "\n",
        "   # 3. Notebooks using this dataset\n",
        "   try:\n",
        "       # Get notebooks that use this dataset across multiple pages\n",
        "       notebooks = api.kernels_list(dataset=ref, page_size=100)\n",
        "       all_notebooks = notebooks.copy()\n",
        "\n",
        "       for page in range(2, 6):  # Check up to 5 pages\n",
        "           try:\n",
        "               more_notebooks = api.kernels_list(dataset=ref, page=page, page_size=100)\n",
        "               if not more_notebooks:\n",
        "                   break\n",
        "               all_notebooks.extend(more_notebooks)\n",
        "           except:\n",
        "               break\n",
        "\n",
        "       # Remove duplicates and sort by votes\n",
        "       unique_notebooks = []\n",
        "       seen_refs = set()\n",
        "       for nb in all_notebooks:\n",
        "           if nb.ref not in seen_refs:\n",
        "               unique_notebooks.append(nb)\n",
        "               seen_refs.add(nb.ref)\n",
        "\n",
        "       sorted_notebooks = sorted(unique_notebooks, key=lambda x: getattr(x, 'voteCount', 0), reverse=True)\n",
        "       total_notebooks_using = len(unique_notebooks)\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Could not retrieve notebooks: {e}\")\n",
        "       total_notebooks_using = 0\n",
        "\n",
        "   # 4. Dataset versions (traceability)\n",
        "   try:\n",
        "       versions = api.dataset_list_versions(ref)\n",
        "       version_info = {\n",
        "           \"total_versions\": len(versions),\n",
        "           \"current_version\": versions[0].versionNumber if versions else 1,\n",
        "           \"version_history\": []\n",
        "       }\n",
        "\n",
        "       # Get details for the 5 most recent versions\n",
        "       for version in versions[:5]:\n",
        "           version_info[\"version_history\"].append({\n",
        "               \"version\": version.versionNumber,\n",
        "               \"creation_date\": getattr(version, \"creationDate\", \"Not available\"),\n",
        "               \"status\": getattr(version, \"status\", \"Not available\")\n",
        "           })\n",
        "   except Exception as e:\n",
        "       print(f\"Could not retrieve version information: {e}\")\n",
        "       version_info = {\"error\": \"Version information not available\"}\n",
        "\n",
        "   # 5. Build the reliability assessment with comprehensive criteria\n",
        "   reliability_assessment = {\n",
        "       \"1_author_info\": {\n",
        "           \"author\": dataset_info[\"author\"],\n",
        "           \"statistics\": author_stats,\n",
        "           \"assessment\": \"✓ Available\" if \"error\" not in author_stats else \"✗ Not available\"\n",
        "       },\n",
        "       \"2_publication_date\": {\n",
        "           \"creation_date\": dataset_info[\"creation_date\"],\n",
        "           \"last_updated\": dataset_info[\"last_updated\"],\n",
        "           \"assessment\": \"✓ Temporal information available\" if dataset_info[\"creation_date\"] != \"Not available\" else \"⚠ Temporal information not available\"\n",
        "       },\n",
        "       \"3_license\": {\n",
        "           \"license\": dataset_info[\"license\"],\n",
        "           \"assessment\": \"⚠ Unknown license\" if dataset_info[\"license\"] == \"Unknown\" else f\"✓ License: {dataset_info['license']}\"\n",
        "       },\n",
        "       \"4_external_source\": {\n",
        "           \"description\": dataset_info[\"description\"],\n",
        "           \"assessment\": \"⚠ No detailed description\" if not dataset_info[\"description\"] else \"✓ Description available\"\n",
        "       },\n",
        "       \"5_traceability\": {\n",
        "           \"versions\": version_info,\n",
        "           \"assessment\": f\"✓ {version_info.get('total_versions', 0)} versions available\" if \"error\" not in version_info else \"⚠ No version information\"\n",
        "       },\n",
        "       \"6_description\": {\n",
        "           \"title\": dataset_info[\"dataset_name\"],\n",
        "           \"subtitle\": dataset_info[\"subtitle\"],\n",
        "           \"description\": dataset_info[\"description\"],\n",
        "           \"assessment\": \"✓ Clear title and subtitle\" if dataset_info[\"subtitle\"] else \"⚠ Limited description\"\n",
        "       },\n",
        "       \"7_community_feedback\": {\n",
        "           \"votes\": dataset_info.get(\"votes\", 0),\n",
        "           \"downloads\": dataset_info.get(\"total_downloads\", 0),\n",
        "           \"featured\": dataset_info.get(\"is_featured\", False),\n",
        "           \"total_notebooks\": total_notebooks_using,\n",
        "           \"assessment\": f\"✓ {dataset_info.get('votes', 0)} votes, {dataset_info.get('total_downloads', 0)} downloads\"\n",
        "       }\n",
        "   }\n",
        "\n",
        "   return {\n",
        "       \"dataset_info\": dataset_info,\n",
        "       \"reliability_assessment\": reliability_assessment\n",
        "   }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_data_source_facet_pipeline():\n",
        "    \"\"\"Ejecuta el pipeline enfocado en Data Facet y Source Facet\"\"\"\n",
        "    \n",
        "    for ref in kernel_refs:\n",
        "        print(f\"\\n[INFO] ========== Processing Data and Source Facets for: {ref} ==========\")\n",
        "        \n",
        "        name = safe_name(ref)\n",
        "        kernel_dir = os.path.join(base_dir, name)\n",
        "        os.makedirs(kernel_dir, exist_ok=True)\n",
        "\n",
        "        # Create directory structure\n",
        "        datasets_dir = Path(kernel_dir) / \"datasets\"\n",
        "        datasets_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        cleaned_dir = datasets_dir / \"cleaned\"\n",
        "        cleaned_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        reports_dir = Path(kernel_dir) / \"reports\"\n",
        "        reports_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # *** AGREGADO: Directorio para visualizaciones ***\n",
        "        visualizations_dir = Path(kernel_dir) / \"visualizations\"\n",
        "        visualizations_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # 1. Download kernel metadata\n",
        "            api.kernels_pull(kernel=ref, path=kernel_dir, metadata=True)\n",
        "            metadata_path = os.path.join(kernel_dir, \"kernel-metadata.json\")\n",
        "            \n",
        "            if not os.path.exists(metadata_path):\n",
        "                print(f\"[WARN] No metadata found for {ref}\")\n",
        "                continue\n",
        "                \n",
        "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                meta = json.load(f)\n",
        "\n",
        "            # 2. SOURCE FACET - Process datasets with reliability analysis\n",
        "            dataset_sources = meta.get(\"dataset_sources\", [])\n",
        "            \n",
        "            for ds_ref in dataset_sources:\n",
        "                try:\n",
        "                    owner, slug = ds_ref.split(\"/\", 1)\n",
        "                except ValueError:\n",
        "                    print(f\"[WARN] Invalid dataset reference: {ds_ref}\")\n",
        "                    continue\n",
        "                    \n",
        "                print(f\"[INFO] Processing dataset: {ds_ref}\")\n",
        "                \n",
        "                # Download dataset\n",
        "                try:\n",
        "                    print(f\"[INFO] Downloading dataset {ds_ref} into {datasets_dir}\")\n",
        "                    api.dataset_download_files(ds_ref, path=str(datasets_dir), unzip=True)\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] Failed to download {ds_ref}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # SOURCE FACET - Run reliability assessment\n",
        "                print(f\"[INFO] Running SOURCE FACET reliability assessment for {ds_ref}...\")\n",
        "                try:\n",
        "                    reliability_eval = dataset_evaluation(ds_ref)\n",
        "                    reliability_path = reports_dir / f\"source_facet_reliability_{slug}.json\"\n",
        "                    safe_json_dump(reliability_eval, reliability_path, indent=2, ensure_ascii=False)\n",
        "                    print(f\"[INFO] SOURCE FACET report saved: {reliability_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[WARN] Could not generate SOURCE FACET report for {ds_ref}: {e}\")\n",
        "\n",
        "            # 3. DATA FACET - Data Quality Analysis\n",
        "            print(f\"[INFO] Running DATA FACET quality analysis...\")\n",
        "            \n",
        "            if datasets_dir.exists() and any(datasets_dir.glob('*.csv')):\n",
        "                # Analyze all CSV files in datasets directory\n",
        "                analysis_report = analyze_multiple_csvs(datasets_dir)\n",
        "\n",
        "                # Save DATA FACET analysis\n",
        "                data_facet_path = reports_dir / \"data_facet_analysis.json\"\n",
        "                safe_json_dump(analysis_report, data_facet_path, indent=2, ensure_ascii=False)\n",
        "                print(f\"[INFO] DATA FACET analysis saved: {data_facet_path}\")\n",
        "\n",
        "                # Process each CSV file for quality and cleaning\n",
        "                quality_decisions = []\n",
        "                \n",
        "                for filename, file_data in analysis_report[\"files_analysis\"].items():\n",
        "                    if file_data.get('errors'):\n",
        "                        print(f\"[WARN] Skipping {filename} due to errors: {file_data['errors']}\")\n",
        "                        continue\n",
        "\n",
        "                    # Calculate quality scores\n",
        "                    scores = calculate_global_score(file_data)\n",
        "                    print(f\"[INFO] DATA FACET quality scores for {filename}: {scores}\")\n",
        "\n",
        "                    dataset_path = datasets_dir / filename\n",
        "                    decision = {\n",
        "                        \"filename\": filename,\n",
        "                        \"quality_scores\": scores,\n",
        "                        \"action_taken\": \"none\"\n",
        "                    }\n",
        "\n",
        "                    # *** AGREGADO: Generate visualizations for original dataset ***\n",
        "                    print(f\"[INFO] Generating visualizations for {filename}...\")\n",
        "                    try:\n",
        "                        plot_histograms(file_data, save_dir=visualizations_dir)\n",
        "                        print(f\"[INFO] Visualizations generated for {filename}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"[WARN] Could not generate visualizations for {filename}: {e}\")\n",
        "\n",
        "                    # Clean dataset if below threshold\n",
        "                    if scores[\"global_score\"] < ALERT_THRESHOLD and dataset_path.exists():\n",
        "                        print(f\"[ALERT] DATA FACET: {filename} ({scores['global_score']:.2f}) below threshold. Cleaning...\")\n",
        "                        try:\n",
        "                            clean_path = clean_dataset(dataset_path, cleaned_dir)\n",
        "                            \n",
        "                            # Re-analyze cleaned dataset\n",
        "                            new_dataset_info = extract_csv_metadata(clean_path)\n",
        "                            new_scores = calculate_global_score(new_dataset_info)\n",
        "                            print(f\"[INFO] DATA FACET: New scores after cleaning: {new_scores}\")\n",
        "                            \n",
        "                            decision.update({\n",
        "                                \"action_taken\": \"cleaned\",\n",
        "                                \"original_score\": scores[\"global_score\"],\n",
        "                                \"cleaned_score\": new_scores[\"global_score\"],\n",
        "                                \"improvement\": new_scores[\"global_score\"] - scores[\"global_score\"]\n",
        "                            })\n",
        "                            \n",
        "                            if new_scores[\"global_score\"] >= ALERT_THRESHOLD:\n",
        "                                print(f\"[SUCCESS] DATA FACET: Cleaning improved {filename}\")\n",
        "                                \n",
        "                                # *** AGREGADO: Generate visualizations for cleaned dataset ***\n",
        "                                print(f\"[INFO] Generating visualizations for cleaned {filename}...\")\n",
        "                                try:\n",
        "                                    plot_histograms(new_dataset_info, save_dir=visualizations_dir)\n",
        "                                    print(f\"[INFO] Visualizations generated for cleaned {filename}\")\n",
        "                                except Exception as e:\n",
        "                                    print(f\"[WARN] Could not generate visualizations for cleaned {filename}: {e}\")\n",
        "                            \n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERROR] DATA FACET: Failed to clean {filename}: {e}\")\n",
        "                            decision[\"action_taken\"] = \"clean_failed\"\n",
        "                            decision[\"error\"] = str(e)\n",
        "                    else:\n",
        "                        if scores[\"global_score\"] >= ALERT_THRESHOLD:\n",
        "                            print(f\"[SUCCESS] DATA FACET: {filename} meets quality standards\")\n",
        "                            decision[\"action_taken\"] = \"passed_quality_check\"\n",
        "\n",
        "                    quality_decisions.append(decision)\n",
        "\n",
        "                # Save DATA FACET quality decisions\n",
        "                quality_decisions_path = reports_dir / \"data_facet_quality_decisions.json\"\n",
        "                safe_json_dump({\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"kernel_ref\": ref,\n",
        "                    \"threshold_used\": ALERT_THRESHOLD,\n",
        "                    \"weights_used\": WEIGHTS,\n",
        "                    \"decisions\": quality_decisions\n",
        "                }, quality_decisions_path, indent=2, ensure_ascii=False)\n",
        "                print(f\"[INFO] DATA FACET quality decisions saved: {quality_decisions_path}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"[WARN] No CSV files found for DATA FACET analysis\")\n",
        "\n",
        "            # Generate combined report\n",
        "            combined_report_path = reports_dir / \"data_source_facet_summary.json\"\n",
        "            combined_report = {\n",
        "                \"pipeline_execution\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"kernel_ref\": ref,\n",
        "                    \"facets_processed\": [\"DATA_FACET\", \"SOURCE_FACET\"]\n",
        "                },\n",
        "                \"data_facet_summary\": {\n",
        "                    \"csv_files_analyzed\": len([f for f in datasets_dir.glob('*.csv')]) if datasets_dir.exists() else 0,\n",
        "                    \"files_cleaned\": len([d for d in quality_decisions if d.get(\"action_taken\") == \"cleaned\"]) if 'quality_decisions' in locals() else 0,\n",
        "                    \"files_passed_quality\": len([d for d in quality_decisions if d.get(\"action_taken\") == \"passed_quality_check\"]) if 'quality_decisions' in locals() else 0,\n",
        "                    # *** AGREGADO: Información sobre visualizaciones ***\n",
        "                    \"visualizations_generated\": len([f for f in visualizations_dir.glob('*.png')]) if visualizations_dir.exists() else 0\n",
        "                },\n",
        "                \"source_facet_summary\": {\n",
        "                    \"datasets_evaluated\": len(dataset_sources),\n",
        "                    \"reliability_reports_generated\": len([f for f in reports_dir.glob('source_facet_reliability_*.json')])\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            safe_json_dump(combined_report, combined_report_path, indent=2, ensure_ascii=False)\n",
        "            print(f\"[INFO] Combined DATA and SOURCE FACET summary saved: {combined_report_path}\")\n",
        "\n",
        "            print(f\"[SUCCESS] ========== Completed Data and Source Facets for {ref} ==========\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to process {ref}: {e}\")\n",
        "\n",
        "    print(f\"\\n[SUCCESS] Data and Source Facet pipeline completed!\")\n",
        "    print(f\"[INFO] Results saved in: {base_dir}/\")\n",
        "    print(f\"[INFO] Each project contains:\")\n",
        "    print(f\"  • DATA FACET: CSV quality analysis, cleaned datasets, and visualizations\")\n",
        "    print(f\"  • SOURCE FACET: Dataset reliability assessments\")\n",
        "    print(f\"  • VISUALIZATIONS: Histogram plots for data distribution analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_data_source_facet_pipeline()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
