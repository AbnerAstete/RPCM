{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Z0sP-3hHy-"
      },
      "source": [
        "# Pipeline for transforming metadata from Kaggle to Research Processes Curation Metamodel (RPCM) implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Gb1V5IhD6C"
      },
      "source": [
        "# Set The Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8r9aZJgJjrD"
      },
      "outputs": [],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-1Pjaox_JVlc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import subprocess\n",
        "import chardet\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "import nbformat\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "import base64\n",
        "import math\n",
        "from sklearn.impute import SimpleImputer\n",
        "from collections import Counter\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkenAn8RTsW_"
      },
      "outputs": [],
      "source": [
        "# For local use, you need to create a .env or set your credentials directly. \n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Set your Kaggle credentials:\n",
        "# os.environ['KAGGLE_USERNAME'] = \"your_username\"\n",
        "# os.environ['KAGGLE_KEY'] = \"your_api_key\"\n",
        "\n",
        "# Authentication\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hZhkntm8TvTP"
      },
      "outputs": [],
      "source": [
        "# Data Quality Configuration\n",
        "WEIGHTS = {\n",
        "    \"completeness\": 0.45,\n",
        "    \"uniqueness\": 0.25,\n",
        "    \"outliers\": 0.30\n",
        "}\n",
        "ALERT_THRESHOLD = 75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z4FjliKTwy2"
      },
      "outputs": [],
      "source": [
        "# List of Kaggle projects\n",
        "kernel_refs = [\n",
        "    #\"joelknapp/student-performance-analysis/\",\n",
        "    \"aremoto/retail-sales-forecast/\",\n",
        "]\n",
        "\n",
        "# Creation of the base directory and complementary functions\n",
        "base_dir = \"kaggle_notebooks\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "def safe_name(ref):\n",
        "    return re.sub(r'[^\\w\\-]', '_', ref)\n",
        "\n",
        "def gen_guid():\n",
        "    return f\"-{uuid.uuid4().int % 1000000000}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiZTHzA3g3sj"
      },
      "source": [
        "# Data Facet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra0b3TtTJW6I"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# DATA QUALITY FUNCTIONS\n",
        "# =====================================================\n",
        "\n",
        "def convert_for_json(obj):\n",
        "   \"\"\"Convert Python objects to JSON serializable format\"\"\"\n",
        "   if isinstance(obj, np.bool_):\n",
        "       return bool(obj)\n",
        "   elif isinstance(obj, np.integer):\n",
        "       return int(obj)\n",
        "   elif isinstance(obj, np.floating):\n",
        "       return float(obj)\n",
        "   elif isinstance(obj, np.ndarray):\n",
        "       return obj.tolist()\n",
        "   elif isinstance(obj, (pd.Timestamp, datetime)):\n",
        "       return obj.isoformat()\n",
        "   elif hasattr(obj, 'item'):\n",
        "       return obj.item()\n",
        "   elif hasattr(obj, 'tolist'):\n",
        "       return obj.tolist()\n",
        "   elif isinstance(obj, dict):\n",
        "       return {k: convert_for_json(v) for k, v in obj.items()}\n",
        "   elif isinstance(obj, list):\n",
        "       return [convert_for_json(item) for item in obj]\n",
        "   return obj\n",
        "\n",
        "def safe_json_dump(data, file_path, **kwargs):\n",
        "   \"\"\"Safely dump data to JSON with proper type conversion\"\"\"\n",
        "   converted_data = convert_for_json(data)\n",
        "   with open(file_path, 'w', encoding='utf-8') as f:\n",
        "       json.dump(converted_data, f, **kwargs)\n",
        "\n",
        "def extract_csv_metadata(file_path):\n",
        "   \"\"\"Extract comprehensive metadata and quality metrics from a CSV file.\"\"\"\n",
        "   file_path = Path(file_path)\n",
        "   metadata = {\n",
        "       'filename': file_path.name,\n",
        "       'file_path': str(file_path),\n",
        "       'analysis_timestamp': datetime.now().isoformat(),\n",
        "       'file_exists': file_path.exists(),\n",
        "       'file_info': {},\n",
        "       'csv_schema': {},\n",
        "       'data_quality': {},\n",
        "       'errors': []\n",
        "   }\n",
        "\n",
        "   if not file_path.exists():\n",
        "       metadata['errors'].append(f\"File not found: {file_path}\")\n",
        "       return metadata\n",
        "\n",
        "   try:\n",
        "       # Extract file system metadata\n",
        "       stat_info = file_path.stat()\n",
        "       metadata['file_info'] = {\n",
        "           'size_bytes': stat_info.st_size,\n",
        "           'size_mb': round(stat_info.st_size / (1024*1024), 2),\n",
        "           'creation_time': datetime.fromtimestamp(stat_info.st_ctime).isoformat(),\n",
        "           'modification_time': datetime.fromtimestamp(stat_info.st_mtime).isoformat(),\n",
        "           'extension': file_path.suffix.lower(),\n",
        "       }\n",
        "\n",
        "       try:\n",
        "           # Read and analyze CSV data\n",
        "           df_full = pd.read_csv(file_path)\n",
        "\n",
        "           # Extract schema information\n",
        "           metadata['csv_schema'] = {\n",
        "               'total_rows': len(df_full),\n",
        "               'total_columns': len(df_full.columns),\n",
        "               'columns': list(df_full.columns),\n",
        "               'column_types': df_full.dtypes.astype(str).to_dict(),\n",
        "               'memory_usage_mb': round(df_full.memory_usage(deep=True).sum() / (1024*1024), 2),\n",
        "               'shape': df_full.shape\n",
        "           }\n",
        "\n",
        "           # Calculate data quality metrics\n",
        "           metadata['data_quality'] = {\n",
        "               'null_values_per_column': df_full.isnull().sum().to_dict(),\n",
        "               'null_percentage_per_column': (df_full.isnull().sum() / len(df_full) * 100).round(2).to_dict(),\n",
        "               'total_null_values': int(df_full.isnull().sum().sum()),\n",
        "               'duplicate_rows': int(df_full.duplicated().sum()),\n",
        "               'unique_values_per_column': df_full.nunique().to_dict(),\n",
        "               'completeness_score': round((1 - df_full.isnull().sum().sum() / df_full.size) * 100, 2)\n",
        "           }\n",
        "\n",
        "           # Extract numeric statistics if numeric columns exist\n",
        "           numeric_columns = df_full.select_dtypes(include=['number']).columns\n",
        "           if len(numeric_columns) > 0:\n",
        "               numeric_stats = df_full[numeric_columns].describe().to_dict()\n",
        "               metadata['numeric_statistics'] = convert_for_json(numeric_stats)\n",
        "\n",
        "       except Exception as e:\n",
        "           metadata['errors'].append(f\"Error reading CSV: {str(e)}\")\n",
        "\n",
        "   except Exception as e:\n",
        "       metadata['errors'].append(f\"Error accessing file: {str(e)}\")\n",
        "\n",
        "   return metadata\n",
        "\n",
        "def analyze_multiple_csvs(directory_path):\n",
        "   \"\"\"Analyze all CSV files in a given directory and generate a comprehensive report.\"\"\"\n",
        "   directory_path = Path(directory_path)\n",
        "   analysis_report = {\n",
        "       'directory': str(directory_path),\n",
        "       'analysis_timestamp': datetime.now().isoformat(),\n",
        "       'csv_files_found': 0,\n",
        "       'files_analysis': {},\n",
        "       'summary': {'errors': []}\n",
        "   }\n",
        "\n",
        "   if not directory_path.exists():\n",
        "       analysis_report['summary']['errors'].append(f\"Directory does not exist: {directory_path}\")\n",
        "       return analysis_report\n",
        "\n",
        "   # Find and analyze all CSV files\n",
        "   csv_files = list(directory_path.glob('*.csv'))\n",
        "   analysis_report['csv_files_found'] = len(csv_files)\n",
        "\n",
        "   for csv_file in csv_files:\n",
        "       print(f\"Analyzing: {csv_file.name}\")\n",
        "       file_metadata = extract_csv_metadata(csv_file)\n",
        "       analysis_report['files_analysis'][csv_file.name] = file_metadata\n",
        "\n",
        "   return analysis_report\n",
        "\n",
        "def calculate_completeness(null_percentage_per_column):\n",
        "   \"\"\"Calculate data completeness score based on null value percentages.\"\"\"\n",
        "   return 100 - np.mean(list(null_percentage_per_column.values()))\n",
        "\n",
        "def calculate_uniqueness(total_rows, duplicate_rows):\n",
        "   \"\"\"Calculate data uniqueness score based on duplicate row count.\"\"\"\n",
        "   return max(0, 100 - (duplicate_rows / total_rows * 100))\n",
        "\n",
        "def calculate_outliers(data):\n",
        "   \"\"\"Calculate outlier penalty score using 3-sigma rule for numeric columns.\"\"\"\n",
        "   penalties = []\n",
        "   for col, stats in data.get(\"numeric_statistics\", {}).items():\n",
        "       mean = stats.get(\"mean\", 0)\n",
        "       std = stats.get(\"std\", 0)\n",
        "       col_min = stats.get(\"min\", mean)\n",
        "       col_max = stats.get(\"max\", mean)\n",
        "       # Apply 3-sigma rule for outlier detection\n",
        "       if std > 0:\n",
        "           if col_min < mean - 3 * std or col_max > mean + 3 * std:\n",
        "               penalties.append(1)\n",
        "           else:\n",
        "               penalties.append(0)\n",
        "   return 100 - (np.mean(penalties) * 100) if penalties else 100\n",
        "\n",
        "def calculate_global_score(dataset):\n",
        "   \"\"\"Calculate overall data quality score using weighted metrics.\"\"\"\n",
        "   completeness = calculate_completeness(dataset[\"data_quality\"][\"null_percentage_per_column\"])\n",
        "   uniqueness = calculate_uniqueness(dataset[\"csv_schema\"][\"total_rows\"], dataset[\"data_quality\"][\"duplicate_rows\"])\n",
        "   outliers = calculate_outliers(dataset)\n",
        "\n",
        "   # Calculate weighted global score\n",
        "   global_score = (\n",
        "       completeness * WEIGHTS[\"completeness\"] +\n",
        "       uniqueness * WEIGHTS[\"uniqueness\"] +\n",
        "       outliers * WEIGHTS[\"outliers\"]\n",
        "   )\n",
        "\n",
        "   return {\n",
        "       \"completeness\": completeness,\n",
        "       \"uniqueness\": uniqueness,\n",
        "       \"outliers\": outliers,\n",
        "       \"global_score\": global_score\n",
        "   }\n",
        "\n",
        "def clean_dataset(file_path, output_dir, lower_quantile=0.05, upper_quantile=0.95, max_missing_frac=0.1):\n",
        "   \"\"\"Clean dataset by handling missing values, duplicates, and outliers.\"\"\"\n",
        "   df = pd.read_csv(file_path)\n",
        "   numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "   # Handle missing values in numeric columns using median imputation\n",
        "   if len(numeric_cols) > 0:\n",
        "       imputer = SimpleImputer(strategy='median')\n",
        "       df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "   # Handle missing values in categorical columns\n",
        "   categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "   for col in categorical_cols:\n",
        "       missing_frac = df[col].isna().mean()\n",
        "       if 0 < missing_frac <= max_missing_frac:\n",
        "           df = df[df[col].notna()]\n",
        "\n",
        "   # Remove duplicate rows\n",
        "   df = df.drop_duplicates()\n",
        "\n",
        "   # Handle outliers using quantile-based clipping\n",
        "   for col in numeric_cols:\n",
        "       lower = df[col].quantile(lower_quantile)\n",
        "       upper = df[col].quantile(upper_quantile)\n",
        "       df[col] = np.clip(df[col], lower, upper)\n",
        "\n",
        "   # Save cleaned dataset\n",
        "   output_dir.mkdir(exist_ok=True)\n",
        "   output_path = output_dir / f\"clean_{Path(file_path).name}\"\n",
        "   df.to_csv(output_path, index=False)\n",
        "   print(f\"Cleaned dataset saved at {output_path}\")\n",
        "   return output_path\n",
        "\n",
        "def plot_histograms(file_data, save_dir=None):\n",
        "   \"\"\"Generate histogram plots for all numeric columns in a dataset with statistical annotations.\"\"\"\n",
        "   file_path = file_data.get('file_path')\n",
        "   filename = file_data.get('filename', 'File')\n",
        "\n",
        "   if not file_path or not Path(file_path).exists():\n",
        "       print(f\"Cannot open {filename} for plotting\")\n",
        "       return\n",
        "\n",
        "   try:\n",
        "       df_full = pd.read_csv(file_path)\n",
        "       \n",
        "       # Select plotting style\n",
        "       available_styles = plt.style.available\n",
        "       preferred_styles = ['seaborn', 'ggplot', 'fivethirtyeight', 'bmh']\n",
        "       selected_style = 'classic'\n",
        "\n",
        "       for style in preferred_styles:\n",
        "           if style in available_styles:\n",
        "               selected_style = style\n",
        "               break\n",
        "\n",
        "       plt.style.use(selected_style)\n",
        "\n",
        "       # Get numeric columns for plotting\n",
        "       numeric_columns = df_full.select_dtypes(include=['number']).columns\n",
        "       n = len(numeric_columns)\n",
        "       if n == 0:\n",
        "           print(\"No numeric columns to plot\")\n",
        "           return\n",
        "\n",
        "       # Set up subplot grid\n",
        "       colors = plt.cm.tab10.colors\n",
        "       cols = min(4, n)\n",
        "       rows = math.ceil(n / cols)\n",
        "       fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4.5))\n",
        "       axes = axes.flatten() if n > 1 else [axes]\n",
        "\n",
        "       # Create histogram for each numeric column\n",
        "       for i, col in enumerate(numeric_columns):\n",
        "           col_data = df_full[col].dropna()\n",
        "           unique_vals = col_data.unique()\n",
        "           num_unique = len(unique_vals)\n",
        "           bins = num_unique if num_unique <= 15 else 'auto'\n",
        "\n",
        "           # Calculate statistical measures\n",
        "           mean_val = col_data.mean()\n",
        "           median_val = col_data.median()\n",
        "           std_val = col_data.std()\n",
        "           skewness = col_data.skew()\n",
        "           kurt = col_data.kurtosis()\n",
        "\n",
        "           # Plot histogram\n",
        "           axes[i].hist(col_data, bins=bins,\n",
        "                        edgecolor='white',\n",
        "                        linewidth=1.2,\n",
        "                        color=colors[i % len(colors)],\n",
        "                        alpha=0.8)\n",
        "\n",
        "           # Add statistical lines\n",
        "           axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:.2f}')\n",
        "           axes[i].axvline(median_val, color='green', linestyle='--', linewidth=1.5, label=f'Median: {median_val:.2f}')\n",
        "\n",
        "           # Customize plot appearance\n",
        "           axes[i].set_title(f\"Distribution of {col}\", pad=15, fontsize=12, fontweight='bold')\n",
        "           axes[i].set_xlabel(col, labelpad=10)\n",
        "           axes[i].set_ylabel('Frequency', labelpad=10)\n",
        "           axes[i].grid(axis='y', alpha=0.3)\n",
        "\n",
        "           # Add statistics text box\n",
        "           stats_text = (f\"Mean: {mean_val:.2f}\\n\"\n",
        "                        f\"Median: {median_val:.2f}\\n\"\n",
        "                        f\"Std. Dev.: {std_val:.2f}\\n\"\n",
        "                        f\"Skewness: {skewness:.2f}\\n\"\n",
        "                        f\"Kurtosis: {kurt:.2f}\")\n",
        "\n",
        "           axes[i].text(0.95, 0.95, stats_text,\n",
        "                        transform=axes[i].transAxes,\n",
        "                        ha='right', va='top',\n",
        "                        bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n",
        "\n",
        "           # Adjust x-axis ticks for discrete variables\n",
        "           if num_unique <= 15:\n",
        "               axes[i].set_xticks(np.linspace(col_data.min(), col_data.max(), num=min(10, num_unique)))\n",
        "\n",
        "           axes[i].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "       # Remove empty subplots\n",
        "       for j in range(i+1, len(axes)):\n",
        "           fig.delaxes(axes[j])\n",
        "\n",
        "       # Add main title and adjust layout\n",
        "       fig.suptitle(f\"Distribution Analysis - {filename}\\n(Style: {selected_style})\",\n",
        "                    y=1.02, fontsize=14, fontweight='bold')\n",
        "       plt.tight_layout()\n",
        "\n",
        "       # Save plot if directory specified\n",
        "       if save_dir:\n",
        "           save_path = save_dir / f\"histogram_{Path(filename).stem}.png\"\n",
        "           plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "           print(f\"Histogram saved: {save_path}\")\n",
        "\n",
        "       plt.close()\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Error plotting {filename}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl1g-8q-g0Is"
      },
      "source": [
        "# Source Facet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3NSHlCSJcm7"
      },
      "outputs": [],
      "source": [
        "def dataset_evaluation(dataset_name):\n",
        "   \"\"\"Evaluate a Kaggle dataset's reliability and gather comprehensive metadata.\"\"\"\n",
        "   owner_slug, dataset_slug = dataset_name.split('/')\n",
        "   ref = f\"{owner_slug}/{dataset_slug}\"\n",
        "\n",
        "   # 1. Basic dataset information\n",
        "   results = api.dataset_list(search=dataset_slug, user=owner_slug)\n",
        "   dataset_info = None\n",
        "\n",
        "   for ds in results:\n",
        "       if ds.ref == ref:\n",
        "           dataset_info = {\n",
        "               \"dataset_name\": ds.title,\n",
        "               \"subtitle\": getattr(ds, \"subtitle\", \"\"),\n",
        "               \"description\": getattr(ds, \"description\", \"\"),\n",
        "               \"license\": getattr(ds, \"license_name\", \"Unknown\"),\n",
        "               \"author\": ds.creator_name,\n",
        "               \"kaggle_id\": ref,\n",
        "               \"kaggle_url\": f\"https://www.kaggle.com/datasets/{ref}\",\n",
        "               \"total_downloads\": ds.download_count,\n",
        "               \"votes\": getattr(ds, 'vote_count', 0),\n",
        "               \"is_private\": ds.is_private,\n",
        "               \"is_featured\": ds.is_featured,\n",
        "               \"usability_rating\": getattr(ds, \"usabilityRating\", None),\n",
        "               \"download_date\": datetime.now().date().isoformat(),\n",
        "               \"creation_date\": getattr(ds, \"creationDate\", \"Not available\"),\n",
        "               \"last_updated\": getattr(ds, \"lastUpdated\", \"Not available\")\n",
        "           }\n",
        "           break\n",
        "\n",
        "   if not dataset_info:\n",
        "       raise ValueError(f\"Dataset {ref} not found\")\n",
        "\n",
        "   # 2. Author information (reputation and activity)\n",
        "   def get_all_datasets(user):\n",
        "       \"\"\"Retrieve all datasets published by a user across multiple pages.\"\"\"\n",
        "       page = 1\n",
        "       all_datasets = []\n",
        "       while True:\n",
        "           try:\n",
        "               datasets = api.dataset_list(user=user, page=page)\n",
        "               if not datasets:\n",
        "                   break\n",
        "               all_datasets.extend(datasets)\n",
        "               page += 1\n",
        "           except:\n",
        "               break\n",
        "       return all_datasets\n",
        "\n",
        "   def get_all_kernels(user):\n",
        "       \"\"\"Retrieve all kernels/notebooks published by a user across multiple pages.\"\"\"\n",
        "       page = 1\n",
        "       all_kernels = []\n",
        "       while True:\n",
        "           try:\n",
        "               kernels = api.kernels_list(user=user, page=page)\n",
        "               if not kernels:\n",
        "                   break\n",
        "               all_kernels.extend(kernels)\n",
        "               page += 1\n",
        "           except:\n",
        "               break\n",
        "       return all_kernels\n",
        "\n",
        "   def get_user_followers(username):\n",
        "       \"\"\"Get user's follower count, following count, and tier information.\"\"\"\n",
        "       try:\n",
        "           user_info = api.user_read(username)\n",
        "           followers = getattr(user_info, \"followerCount\", \"Not available\")\n",
        "           following = getattr(user_info, \"followingCount\", \"Not available\")\n",
        "           tier = getattr(user_info, \"tier\", \"Not available\")\n",
        "           return followers, following, tier\n",
        "       except:\n",
        "           return \"Not available\", \"Not available\", \"Not available\"\n",
        "\n",
        "   try:\n",
        "       # Gather comprehensive author statistics\n",
        "       author_datasets = get_all_datasets(owner_slug)\n",
        "       total_datasets = len(author_datasets)\n",
        "       author_notebooks = get_all_kernels(owner_slug)\n",
        "       total_notebooks = len(author_notebooks)\n",
        "       followers, following, tier = get_user_followers(owner_slug)\n",
        "\n",
        "       # Calculate aggregate metrics\n",
        "       total_downloads = sum(getattr(ds, 'download_count', 0) for ds in author_datasets)\n",
        "       total_votes = sum(getattr(ds, 'vote_count', 0) for ds in author_datasets)\n",
        "       notebook_votes = sum(getattr(nb, 'voteCount', 0) for nb in author_notebooks)\n",
        "\n",
        "       author_stats = {\n",
        "           \"total_datasets\": total_datasets,\n",
        "           \"total_notebooks\": total_notebooks,\n",
        "           \"total_dataset_downloads\": total_downloads,\n",
        "           \"total_dataset_votes\": total_votes,\n",
        "           \"total_notebook_votes\": notebook_votes,\n",
        "           \"follower_count\": followers,\n",
        "           \"following_count\": following,\n",
        "           \"author_tier\": tier,\n",
        "           \"avg_downloads_per_dataset\": round(total_downloads / total_datasets, 2) if total_datasets > 0 else 0,\n",
        "           \"avg_votes_per_dataset\": round(total_votes / total_datasets, 2) if total_datasets > 0 else 0\n",
        "       }\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Could not retrieve author statistics: {e}\")\n",
        "       author_stats = {\"error\": \"Author information not available\"}\n",
        "\n",
        "   # 3. Notebooks using this dataset\n",
        "   try:\n",
        "       # Get notebooks that use this dataset across multiple pages\n",
        "       notebooks = api.kernels_list(dataset=ref, page_size=100)\n",
        "       all_notebooks = notebooks.copy()\n",
        "\n",
        "       for page in range(2, 6):  # Check up to 5 pages\n",
        "           try:\n",
        "               more_notebooks = api.kernels_list(dataset=ref, page=page, page_size=100)\n",
        "               if not more_notebooks:\n",
        "                   break\n",
        "               all_notebooks.extend(more_notebooks)\n",
        "           except:\n",
        "               break\n",
        "\n",
        "       # Remove duplicates and sort by votes\n",
        "       unique_notebooks = []\n",
        "       seen_refs = set()\n",
        "       for nb in all_notebooks:\n",
        "           if nb.ref not in seen_refs:\n",
        "               unique_notebooks.append(nb)\n",
        "               seen_refs.add(nb.ref)\n",
        "\n",
        "       sorted_notebooks = sorted(unique_notebooks, key=lambda x: getattr(x, 'voteCount', 0), reverse=True)\n",
        "       total_notebooks_using = len(unique_notebooks)\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Could not retrieve notebooks: {e}\")\n",
        "       total_notebooks_using = 0\n",
        "\n",
        "   # 4. Dataset versions (traceability)\n",
        "   try:\n",
        "       versions = api.dataset_list_versions(ref)\n",
        "       version_info = {\n",
        "           \"total_versions\": len(versions),\n",
        "           \"current_version\": versions[0].versionNumber if versions else 1,\n",
        "           \"version_history\": []\n",
        "       }\n",
        "\n",
        "       # Get details for the 5 most recent versions\n",
        "       for version in versions[:5]:\n",
        "           version_info[\"version_history\"].append({\n",
        "               \"version\": version.versionNumber,\n",
        "               \"creation_date\": getattr(version, \"creationDate\", \"Not available\"),\n",
        "               \"status\": getattr(version, \"status\", \"Not available\")\n",
        "           })\n",
        "   except Exception as e:\n",
        "       print(f\"Could not retrieve version information: {e}\")\n",
        "       version_info = {\"error\": \"Version information not available\"}\n",
        "\n",
        "   # 5. Build the reliability assessment with comprehensive criteria\n",
        "   reliability_assessment = {\n",
        "       \"1_author_info\": {\n",
        "           \"author\": dataset_info[\"author\"],\n",
        "           \"statistics\": author_stats,\n",
        "           \"assessment\": \"✓ Available\" if \"error\" not in author_stats else \"✗ Not available\"\n",
        "       },\n",
        "       \"2_publication_date\": {\n",
        "           \"creation_date\": dataset_info[\"creation_date\"],\n",
        "           \"last_updated\": dataset_info[\"last_updated\"],\n",
        "           \"assessment\": \"✓ Temporal information available\" if dataset_info[\"creation_date\"] != \"Not available\" else \"⚠ Temporal information not available\"\n",
        "       },\n",
        "       \"3_license\": {\n",
        "           \"license\": dataset_info[\"license\"],\n",
        "           \"assessment\": \"⚠ Unknown license\" if dataset_info[\"license\"] == \"Unknown\" else f\"✓ License: {dataset_info['license']}\"\n",
        "       },\n",
        "       \"4_external_source\": {\n",
        "           \"description\": dataset_info[\"description\"],\n",
        "           \"assessment\": \"⚠ No detailed description\" if not dataset_info[\"description\"] else \"✓ Description available\"\n",
        "       },\n",
        "       \"5_traceability\": {\n",
        "           \"versions\": version_info,\n",
        "           \"assessment\": f\"✓ {version_info.get('total_versions', 0)} versions available\" if \"error\" not in version_info else \"⚠ No version information\"\n",
        "       },\n",
        "       \"6_description\": {\n",
        "           \"title\": dataset_info[\"dataset_name\"],\n",
        "           \"subtitle\": dataset_info[\"subtitle\"],\n",
        "           \"description\": dataset_info[\"description\"],\n",
        "           \"assessment\": \"✓ Clear title and subtitle\" if dataset_info[\"subtitle\"] else \"⚠ Limited description\"\n",
        "       },\n",
        "       \"7_community_feedback\": {\n",
        "           \"votes\": dataset_info.get(\"votes\", 0),\n",
        "           \"downloads\": dataset_info.get(\"total_downloads\", 0),\n",
        "           \"featured\": dataset_info.get(\"is_featured\", False),\n",
        "           \"total_notebooks\": total_notebooks_using,\n",
        "           \"assessment\": f\"✓ {dataset_info.get('votes', 0)} votes, {dataset_info.get('total_downloads', 0)} downloads\"\n",
        "       }\n",
        "   }\n",
        "\n",
        "   return {\n",
        "       \"dataset_info\": dataset_info,\n",
        "       \"reliability_assessment\": reliability_assessment\n",
        "   }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eTyXRkkgsnu"
      },
      "source": [
        "# Extract Kaggle Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extraction of Kaggle Project Outputs - Logs\n",
        "\n",
        "- Stdout extraction: Extracts text from standard output stream using regex and line analysis.\n",
        "- Dataset metrics: Captures rows, columns, data types, and memory usage.\n",
        "- Model performance: Extracts model name, accuracy, confusion matrix, and classification metrics.\n",
        "- Execution times: Collects kernel runtime and performance data.\n",
        "- File metadata: Gathers file size, creation date, and line count.\n",
        "- JSON output: Consolidates data into structured log_analysis.json format.\n",
        "\n",
        "#### Extraction of Insights from Jupyter Notebooks\n",
        "\n",
        "- Dataset detection: Identifies loaded datasets from pd.read_csv calls with input folder paths.\n",
        "- Model identification: Extracts referenced ML models like XGBClassifier, SVC, and other classifiers.\n",
        "- Metrics extraction: Collects evaluation metrics including accuracy, F1 score, confusion matrix, and classification reports.\n",
        "- Visual output detection: Identifies graphs through .show() calls and embedded images in output cells.\n",
        "- Section tracking: Maps insights to notebook sections based on markdown headers.\n",
        "- Structured output: Aggregates data into dictionary format with sets converted to lists for JSON serialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mP22We5JsUe"
      },
      "outputs": [],
      "source": [
        "def download_kaggle_output(kernel_ref: str, download_path: str):\n",
        "   \"\"\"Download outputs from a Kaggle kernel to specified path.\"\"\"\n",
        "   print(f\"[INFO] Downloading outputs from: {kernel_ref}\")\n",
        "   subprocess.run([\n",
        "       \"kaggle\", \"kernels\", \"output\", kernel_ref,\n",
        "       \"-p\", download_path\n",
        "   ], check=True)\n",
        "   print(f\"[INFO] Download completed at: {download_path}\")\n",
        "\n",
        "def detect_encoding(file_path):\n",
        "   \"\"\"Detect the encoding of a file\"\"\"\n",
        "   with open(file_path, 'rb') as f:\n",
        "       return chardet.detect(f.read(100000))['encoding']\n",
        "\n",
        "def load_log_json(file_path):\n",
        "   \"\"\"Load JSON log file with proper encoding detection.\"\"\"\n",
        "   encoding = detect_encoding(file_path)\n",
        "   with open(file_path, 'r', encoding=encoding) as f:\n",
        "       return json.load(f), encoding\n",
        "\n",
        "def extract_text_from_stdout(log_data):\n",
        "   \"\"\"Extract all stdout text from log data entries.\"\"\"\n",
        "   return ''.join(entry['data'] for entry in log_data if entry['stream_name'] == 'stdout')\n",
        "\n",
        "def extract_dataset_info(text):\n",
        "   \"\"\"Parse dataset information from text output including rows, columns, and data types.\"\"\"\n",
        "   info = {}\n",
        "   match = re.search(r\"RangeIndex: (\\d+)\", text)\n",
        "   if match:\n",
        "       info[\"rows\"] = int(match.group(1))\n",
        "   match = re.search(r\"Data columns \\(total (\\d+) columns\\):\", text)\n",
        "   if match:\n",
        "       info[\"columns\"] = int(match.group(1))\n",
        "   dtypes = re.findall(r\"\\d+\\s+(\\w+)\\s+\\d+ non-null\\s+(\\w+)\", text)\n",
        "   info[\"dtypes\"] = {col: dtype for col, dtype in dtypes}\n",
        "   match = re.search(r\"memory usage:\\s+([^\\n]+)\", text)\n",
        "   if match:\n",
        "       info[\"memory_usage\"] = match.group(1)\n",
        "   return info\n",
        "\n",
        "def extract_models_info(text):\n",
        "   \"\"\"Extract machine learning models performance information from text output.\"\"\"\n",
        "   lines = text.splitlines()\n",
        "   models = []\n",
        "   current = {\n",
        "       \"model\": None,\n",
        "       \"accuracy\": None,\n",
        "       \"conf_matrix\": [],\n",
        "       \"class_metrics\": {},\n",
        "       \"avg_metrics\": {},\n",
        "   }\n",
        "\n",
        "   state = None\n",
        "   pending_matrix = []\n",
        "\n",
        "   for line in lines:\n",
        "       line = line.strip()\n",
        "\n",
        "       if not line:\n",
        "           continue\n",
        "\n",
        "       if line.lower().startswith(\"confussion matrix\"):\n",
        "           state = \"conf_matrix\"\n",
        "           pending_matrix = []\n",
        "           continue\n",
        "       elif state == \"conf_matrix\" and \"[\" in line:\n",
        "           numbers = list(map(int, re.findall(r\"\\d+\", line)))\n",
        "           if len(numbers) == 2:\n",
        "               pending_matrix.append(numbers)\n",
        "           if len(pending_matrix) == 2:\n",
        "               current[\"conf_matrix\"] = pending_matrix\n",
        "               state = None\n",
        "           continue\n",
        "\n",
        "       match = re.match(r\"Accuracy of (.*?):\\s*([\\d.]+)\", line)\n",
        "       if match:\n",
        "           current[\"model\"] = match.group(1).strip()\n",
        "           current[\"accuracy\"] = float(match.group(2))\n",
        "           continue\n",
        "\n",
        "       if re.match(r\"^precision\\s+recall\\s+f1-score\\s+support\", line):\n",
        "           state = \"report\"\n",
        "           continue\n",
        "\n",
        "       if state == \"report\":\n",
        "           parts = re.split(r\"\\s+\", line)\n",
        "           if len(parts) == 5:\n",
        "               label = parts[0]\n",
        "               try:\n",
        "                   metrics = {\n",
        "                       \"precision\": float(parts[1]),\n",
        "                       \"recall\": float(parts[2]),\n",
        "                       \"f1-score\": float(parts[3]),\n",
        "                       \"support\": int(parts[4]),\n",
        "                   }\n",
        "                   if label in (\"0\", \"1\"):\n",
        "                       current[\"class_metrics\"][label] = metrics\n",
        "                   elif label in (\"macro\", \"weighted\"):\n",
        "                       current[\"avg_metrics\"][label + \" avg\"] = metrics\n",
        "               except ValueError:\n",
        "                   pass\n",
        "           elif \"accuracy\" in line.lower():\n",
        "               state = None\n",
        "               models.append(current)\n",
        "               current = {\n",
        "                   \"model\": None,\n",
        "                   \"accuracy\": None,\n",
        "                   \"conf_matrix\": [],\n",
        "                   \"class_metrics\": {},\n",
        "                   \"avg_metrics\": {},\n",
        "               }\n",
        "\n",
        "   if current[\"model\"] and current not in models:\n",
        "       models.append(current)\n",
        "\n",
        "   return models\n",
        "\n",
        "def extract_execution_time(log_data):\n",
        "   \"\"\"Calculate execution time from log data timestamps.\"\"\"\n",
        "   times = [entry[\"time\"] for entry in log_data if \"time\" in entry]\n",
        "   if not times:\n",
        "       return {}\n",
        "   return {\n",
        "       \"start\": times[0],\n",
        "       \"end\": times[-1],\n",
        "       \"duration_seconds\": round(times[-1] - times[0], 3)\n",
        "   }\n",
        "\n",
        "def extract_file_metadata(file_path, encoding):\n",
        "   \"\"\"Extract file metadata including size, creation time, and line count.\"\"\"\n",
        "   stat = os.stat(file_path)\n",
        "   with open(file_path, 'r', encoding=encoding) as f:\n",
        "       num_lines = sum(1 for _ in f)\n",
        "   return {\n",
        "       \"filename\": os.path.basename(file_path),\n",
        "       \"filepath\": os.path.abspath(file_path),\n",
        "       \"created_at\": datetime.fromtimestamp(stat.st_ctime).isoformat(),\n",
        "       \"num_lines\": num_lines,\n",
        "       \"encoding\": encoding,\n",
        "       \"total_bytes\": stat.st_size\n",
        "   }\n",
        "\n",
        "def analyze_kaggle_log(kernel_ref: str, output_path: str):\n",
        "   \"\"\"Main function to download and analyze Kaggle kernel log files.\"\"\"\n",
        "   download_kaggle_output(kernel_ref, output_path)\n",
        "\n",
        "   log_file = next((f for f in os.listdir(output_path) if f.endswith('.log')), None)\n",
        "   if not log_file:\n",
        "       raise FileNotFoundError(\"No .log file found in the download.\")\n",
        "   log_path = os.path.join(output_path, log_file)\n",
        "\n",
        "   log_data, encoding = load_log_json(log_path)\n",
        "   text = extract_text_from_stdout(log_data)\n",
        "\n",
        "   result = {\n",
        "       \"file_info\": extract_file_metadata(log_path, encoding),\n",
        "       \"dataset_info\": extract_dataset_info(text),\n",
        "       \"models\": extract_models_info(text),\n",
        "       \"execution_time\": extract_execution_time(log_data)\n",
        "   }\n",
        "\n",
        "   safe_json_dump(result, os.path.join(output_path, \"log_analysis.json\"), indent=2, ensure_ascii=False)\n",
        "\n",
        "   return result\n",
        "\n",
        "def extract_insights_from_notebook(notebook_path, project_id=None):\n",
        "   \"\"\"Extract insights from Jupyter notebook including datasets, models, metrics and visualizations.\"\"\"\n",
        "   with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "       nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "   insights = {\n",
        "       \"datasets\": set(),\n",
        "       \"models\": set(),\n",
        "       \"metrics\": set(),\n",
        "       \"graphs\": [],\n",
        "       \"sections\": []\n",
        "   }\n",
        "\n",
        "   current_section = \"Unknown\"\n",
        "   figure_count = 1\n",
        "   seen_graphs = set()\n",
        "   model_by_cell = []\n",
        "\n",
        "   for idx, cell in enumerate(nb.cells):\n",
        "       if cell.cell_type == \"markdown\":\n",
        "           headers = re.findall(r'^#{1,6}\\s+(.*)', cell.source, re.MULTILINE)\n",
        "           if headers:\n",
        "               current_section = headers[0].strip()\n",
        "               insights[\"sections\"].append(current_section)\n",
        "\n",
        "       if cell.cell_type == \"code\":\n",
        "           code = cell.source\n",
        "\n",
        "           # Extract CSV dataset paths\n",
        "           csv_paths = re.findall(r\"pd\\.read_csv\\(['\\\"](.+?/input/[^/]+/[^'\\\"]+)['\\\"]\", code)\n",
        "           for path in csv_paths:\n",
        "               file_match = re.search(r\"/([^/]+\\.csv)\", path)\n",
        "               if file_match:\n",
        "                   insights[\"datasets\"].add(file_match.group(1))\n",
        "\n",
        "           # Extract machine learning models\n",
        "           models_found = re.findall(r'\\b([A-Z][a-zA-Z]+Classifier|SVC|XGBClassifier)\\b', code)\n",
        "           if models_found:\n",
        "               insights[\"models\"].update(models_found)\n",
        "           model_by_cell.append(models_found)\n",
        "\n",
        "           # Extract evaluation metrics\n",
        "           for m in [\"accuracy_score\", \"f1_score\", \"confusion_matrix\", \"classification_report\"]:\n",
        "               if m in code:\n",
        "                   insights[\"metrics\"].add(m)\n",
        "\n",
        "           # Detect visualizations\n",
        "           has_show = \".show()\" in code\n",
        "           has_output = any(\n",
        "               out.get(\"output_type\") == \"display_data\" and \"image/png\" in out.get(\"data\", {})\n",
        "               for out in cell.get(\"outputs\", [])\n",
        "           )\n",
        "\n",
        "           if has_show or has_output:\n",
        "               model_for_graph = models_found[0] if models_found else None\n",
        "\n",
        "               # Look for model in previous cells if not found in current cell\n",
        "               if not model_for_graph:\n",
        "                   for prev_models in reversed(model_by_cell[:-1]):\n",
        "                       if prev_models:\n",
        "                           model_for_graph = prev_models[0]\n",
        "                           break\n",
        "\n",
        "               graph_signature = (project_id, current_section, model_for_graph)\n",
        "               if graph_signature not in seen_graphs:\n",
        "                   insights[\"graphs\"].append({\n",
        "                       \"name\": f\"Figure {figure_count}\",\n",
        "                       \"section\": current_section,\n",
        "                       \"model\": model_for_graph if model_for_graph else \"Unknown\"\n",
        "                   })\n",
        "                   seen_graphs.add(graph_signature)\n",
        "                   figure_count += 1\n",
        "\n",
        "   # Convert sets to lists for JSON serialization\n",
        "   insights[\"datasets\"] = list(insights[\"datasets\"])\n",
        "   insights[\"models\"] = list(insights[\"models\"])\n",
        "   insights[\"metrics\"] = list(insights[\"metrics\"])\n",
        "\n",
        "   return insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-SQDFAkgo8i"
      },
      "source": [
        "# Transformations Kaggle Entities to RPCM Entities\n",
        "\n",
        "This function is designed to transform a structured input dictionary (extracted metadata from Kaggle) into a collection of entities formatted to be ingested into the ATLAS metadata repository.  \n",
        "\n",
        "The output is a **list of dictionaries**, each representing an ATLAS entity with assigned types, GUIDs. This bulk of entities is then ready to be ingested by ATLAS, enabling tracking and governance of data science workflows with provenance and lineage.\n",
        "\n",
        "### Step-by-step breakdown\n",
        "\n",
        "1. Helper Functions & GUID Generation\n",
        "The function begins with robust helper functions for data validation:\n",
        "\n",
        "- generate_qualified_name(): Creates unique, Atlas-compatible qualified names with sanitized characters\n",
        "- safe_timestamp(): Generates valid timestamps in milliseconds, avoiding format issues\n",
        "- safe_size(): Converts size values to integers with fallback defaults, handles null/invalid values\n",
        "- clean_format(): Sanitizes file format strings, removes problematic characters like leading dots\n",
        "\n",
        "- Then generates unique identifiers (GUIDs) for each primary entity type.These GUIDs serve as immutable unique references linking entities and establishing relationships.\n",
        "\n",
        "\n",
        "2. User Entity\n",
        "\n",
        "- Constructed using the owner information from the input JSON.\n",
        "- If the owner's name is missing → defaults to \"anonymous\".\n",
        "- Assigned a hardcoded role: \"Kaggle Contributor\".\n",
        "- Uses sanitized qualified names for Atlas compatibility.\n",
        "\n",
        "3. Project Entity\n",
        "\n",
        "- Project title: defaults to \"Untitled Project\".\n",
        "- Keywords: extracted from project data, defaults to [].\n",
        "- createdBy: links to the user GUID.\n",
        "- Start date: uses safe_timestamp() for proper millisecond format.\n",
        "- References an Experiment entity through relationshipAttributes.\n",
        "\n",
        "4. Experiment Entity\n",
        "\n",
        "- Linked to the project with proper GUID references.\n",
        "- Name uses the project title directly (simplified naming).\n",
        "- References one Stage entity through relationshipAttributes.\n",
        "\n",
        "5. Stage Entity\n",
        "\n",
        "- Represents a specific phase in the experiment workflow.\n",
        "- Name: \"Stage\" (simplified).\n",
        "- Status: \"Completed\".\n",
        "- References one Iteration entity through relationshipAttributes.\n",
        "\n",
        "6. Iteration Entity\n",
        "\n",
        "- Name = \"Iteration {iteration_number}\" (default = 1).\n",
        "- References its parent stage by GUID.\n",
        "- Uses safe qualified name generation.\n",
        "\n",
        "7. Used Data Entities (Inputs) \n",
        "\n",
        "- Processes datasets and files with improved data handling:\n",
        "\n",
        "  - File items: From \"File\" array with safe size conversion and format cleaning\n",
        "  - DataSets: Additional dataset processing capability\n",
        "  - Creates a UsedData entity for each with: Safe size conversion using safe_size() with 50KB default. Format cleaning via clean_format()\n",
        "  - Descriptive names: \"Dataset: {filename}\"\n",
        "  - All collected into inputs list for Action relationship.\n",
        "\n",
        "8. Used Data Entity for Notebook - As Output\n",
        "\n",
        "- Notebook file treated as output UsedData (workflow result).\n",
        "- Enhanced attributes:\n",
        "\n",
        "  - Proper format extraction with clean_format()\n",
        "  - Default size of 85KB using safe_size()\n",
        "  - Descriptive name: \"Notebook: {filename}\"\n",
        "\n",
        "9. Used Data Entity for Log \n",
        "\n",
        "- Conditional processing: Only creates entity if log data exists\n",
        "- Robust field extraction:\n",
        "  - Filename with fallback to \"output.log\"\n",
        "  - Size conversion using safe_size()\n",
        "  - Format cleaning for proper Atlas ingestion\n",
        "- Added to outputs list.\n",
        "\n",
        "10. Used Data Entities for Graphs \n",
        "\n",
        "- Extracted from \"graphs\" key in \"CodeLine\".\n",
        "- Parses graph names, handles \"-\" separators\n",
        "- Sanitizes spaces to underscores\n",
        "- Auto-appends .png extension\n",
        "\n",
        "- Consistent attributes:\n",
        "\n",
        "  - Format: \"png\" (without leading dot)\n",
        "  - Default size: 45KB via safe_size()\n",
        "  - Clean qualified names\n",
        "\n",
        "\n",
        "11. Used Data Entities for Models \n",
        "\n",
        "- Flexible extraction: From \"Model\" array OR \"CodeLine.models\"\n",
        "- Document names: \"{model_name}.pickle\"\n",
        "- Format: \"pickle\"\n",
        "- Default size: 1MB for realistic model files\n",
        "- All linked as outputs.\n",
        "\n",
        "12. Action Entity \n",
        "\n",
        "- Represents execution with detailed input/output tracking:\n",
        "  - inputData: String description of analyzed datasets (not array)\n",
        "  - outputData: Summary of generated outputs count\n",
        "  - Name: \"Action - Notebook - {project_title}\"\n",
        "  - madeBy as array with user reference\n",
        "- References inputs/outputs through relationshipAttributes.\n",
        "\n",
        "13. Workgroup Entity\n",
        "\n",
        "- Name: \"Workgroup - {project_title}\"\n",
        "- Description: \"Workgroup - {project_title}\" (as requested)\n",
        "- Users: Array containing the project creator/owner\n",
        "- Experiment: Links to the experiment entity via GUID reference\n",
        "\n",
        "14. Consensus Entity\n",
        "- typeConsensus: \"Individual Review\"\n",
        "- agreementLevel: 95 (numeric)\n",
        "- result: \"approved\" (lowercase for schema compliance)\n",
        "- Proper GUID linking to action and user entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P9hWipJJ752"
      },
      "outputs": [],
      "source": [
        "def build_atlas_bulk_from_json(input_json: dict, iteration_number=1):\n",
        "    entities = []\n",
        "\n",
        "    # === HELPER FUNCTIONS ===\n",
        "    def generate_qualified_name(entity_type, name, project_name):\n",
        "        \"\"\"Generate a unique and valid qualifiedName\"\"\"\n",
        "        safe_name = str(name).lower().replace(\" \", \"-\").replace(\"_\", \"-\")[:50]\n",
        "        safe_project = str(project_name).replace(\" \", \"\").replace(\"_\", \"\")[:20]\n",
        "        return f\"{safe_name}@{safe_project}\"\n",
        "\n",
        "    def safe_timestamp():\n",
        "        \"\"\"Generates a valid timestamp in milliseconds\"\"\"\n",
        "        return int(datetime.today().timestamp() * 1000)\n",
        "\n",
        "    def safe_size(size_value, default=50000):\n",
        "        \"\"\"Convert size to valid integer, avoid None/null\"\"\"\n",
        "        if size_value is None or size_value == \"null\":\n",
        "            return default\n",
        "        try:\n",
        "            return int(size_value) if size_value else default\n",
        "        except (ValueError, TypeError):\n",
        "            return default\n",
        "\n",
        "    def clean_format(file_format):\n",
        "        \"\"\"Clean file format (no leading dots)\"\"\"\n",
        "        if not file_format:\n",
        "            return \"unknown\"\n",
        "        return str(file_format).lstrip(\".\")\n",
        "\n",
        "    # === GENERATE GUIDS ===\n",
        "    guids = {\n",
        "        \"user\": gen_guid(),\n",
        "        \"project\": gen_guid(),\n",
        "        \"experiment\": gen_guid(),\n",
        "        \"workgroup\": gen_guid(),\n",
        "        \"stage\": gen_guid(),\n",
        "        \"iteration\": gen_guid(),\n",
        "        \"action\": gen_guid()\n",
        "    }\n",
        "\n",
        "    # === EXTRACT PROJECT INFO ===\n",
        "    project_data = input_json.get(\"Project\", {})\n",
        "    project_title = project_data.get(\"title\", \"Untitled Project\")\n",
        "    project_safe_name = project_title.replace(\" \", \"\").replace(\"-\", \"\")\n",
        "\n",
        "    # === USER ===\n",
        "    owner = input_json.get(\"Owner\", {})\n",
        "    user_name = owner.get(\"name\", \"anonymous\")\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"User\",\n",
        "        \"guid\": guids[\"user\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": user_name,\n",
        "            \"role\": \"Kaggle Contributor\",\n",
        "            \"qualifiedName\": generate_qualified_name(\"user\", user_name, project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === PROJECT ===\n",
        "    project_keywords = project_data.get(\"keywords\", [])\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"Project\",\n",
        "        \"guid\": guids[\"project\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": project_title,\n",
        "            \"keywords\": project_keywords,\n",
        "            \"createdBy\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "            \"startDate\": safe_timestamp(),\n",
        "            \"qualifiedName\": generate_qualified_name(\"project\", project_title, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"experiments\": [{\"guid\": guids[\"experiment\"], \"typeName\": \"Experiment\"}]\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === WORKGROUP ===\n",
        "    workgroup_name = f\"Workgroup - {project_title}\"\n",
        "    workgroup_description = f\"Workgroup - {project_title}\"\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"Workgroup\",\n",
        "        \"guid\": guids[\"workgroup\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": workgroup_name,\n",
        "            \"description\": workgroup_description,\n",
        "            \"experiment\": {\"guid\": guids[\"experiment\"], \"typeName\": \"Experiment\"},\n",
        "            \"users\": [{\"guid\": guids[\"user\"], \"typeName\": \"User\"}],\n",
        "            \"qualifiedName\": generate_qualified_name(\"workgroup\", workgroup_name, project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === EXPERIMENT ===\n",
        "    experiment_title = f\"{project_title} - Experiment\"\n",
        "    entities.append({\n",
        "        \"typeName\": \"Experiment\",\n",
        "        \"guid\": guids[\"experiment\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"{project_title}\",\n",
        "            \"project\": {\"guid\": guids[\"project\"], \"typeName\": \"Project\"},\n",
        "            \"qualifiedName\": generate_qualified_name(\"experiment\", experiment_title, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"stages\": [{\"guid\": guids[\"stage\"], \"typeName\": \"Stage\"}],\n",
        "            \"workgroup\": [{\"guid\": guids[\"workgroup\"], \"typeName\": \"Workgroup\"}]\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === STAGE ===\n",
        "    stage_name = \"Stage\"\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"Stage\",\n",
        "        \"guid\": guids[\"stage\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": stage_name,\n",
        "            \"experiment\": {\"guid\": guids[\"experiment\"], \"typeName\": \"Experiment\"},\n",
        "            \"status\": \"Completed\",\n",
        "            \"qualifiedName\": generate_qualified_name(\"stage\", stage_name, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"iterations\": [{\"guid\": guids[\"iteration\"], \"typeName\": \"Iteration\"}]\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === ITERATION ===\n",
        "    iteration_name = f\"Iteration {iteration_number}\"\n",
        "    entities.append({\n",
        "        \"typeName\": \"Iteration\",\n",
        "        \"guid\": guids[\"iteration\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": iteration_name,\n",
        "            \"stage\": {\"guid\": guids[\"stage\"], \"typeName\": \"Stage\"},\n",
        "            \"qualifiedName\": generate_qualified_name(\"iteration\", iteration_name, project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === COLLECT INPUTS/OUTPUTS ===\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "\n",
        "    # === USED DATA (from File + DataSets) ===\n",
        "    file_items = input_json.get(\"File\", [])\n",
        "    datasets = input_json.get(\"DataSets\", [])\n",
        "\n",
        "    for i, file in enumerate(file_items):\n",
        "        useddata_guid = gen_guid()\n",
        "\n",
        "        file_name = file.get(\"name\", f\"file_{i}\")\n",
        "        file_size = safe_size(file.get(\"totalbytes\"))\n",
        "        file_format = clean_format(Path(file_name).suffix or \"csv\")\n",
        "        quality_info = file.get(\"quality_info\", {})\n",
        "\n",
        "        # Enhanced name to indicate quality status with specific CSV cleaning info\n",
        "        dataset_name = f\"Dataset {i+1}: {file_name}\"\n",
        "        cleaning_status = \"\"\n",
        "\n",
        "        if quality_info.get(\"is_cleaned_version\"):\n",
        "            dataset_name += \" (Quality Enhanced - CSV Cleaned)\"\n",
        "            cleaning_status = f\"CSV '{file_name}' was cleaned and quality enhanced\"\n",
        "        elif quality_info.get(\"meets_threshold\"):\n",
        "            dataset_name += \" (Quality Verified - Original CSV)\"\n",
        "            cleaning_status = f\"CSV '{file_name}' met quality standards, original version used\"\n",
        "        else:\n",
        "            cleaning_status = f\"CSV '{file_name}' - no quality analysis performed\"\n",
        "\n",
        "        # Add quality metadata to the entity\n",
        "        entity_attributes = {\n",
        "            \"name\": dataset_name,\n",
        "            \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "            \"document\": file_name,\n",
        "            \"format\": file_format,\n",
        "            \"size\": file_size,\n",
        "            \"qualifiedName\": generate_qualified_name(\"data\", f\"dataset-{i}-{file_name}\", project_safe_name),\n",
        "            \"dataProcessingStatus\": cleaning_status  # New field to explicitly mention cleaning\n",
        "        }\n",
        "\n",
        "        # Add quality metrics if available\n",
        "        if quality_info.get(\"quality_score\") is not None:\n",
        "            entity_attributes[\"qualityScore\"] = round(quality_info[\"quality_score\"], 2)\n",
        "            entity_attributes[\"qualityStatus\"] = \"Enhanced\" if quality_info.get(\"is_cleaned_version\") else \"Verified\"\n",
        "            entity_attributes[\"meetsQualityThreshold\"] = quality_info.get(\"meets_threshold\", False)\n",
        "            entity_attributes[\"csvCleaningApplied\"] = quality_info.get(\"is_cleaned_version\", False)\n",
        "\n",
        "            # Add specific cleaning details\n",
        "            if quality_info.get(\"is_cleaned_version\"):\n",
        "                entity_attributes[\"cleaningDetails\"] = f\"Original CSV '{file_name}' was processed and cleaned due to quality score below threshold. Enhanced version is being used for analysis.\"\n",
        "            else:\n",
        "                entity_attributes[\"cleaningDetails\"] = f\"Original CSV '{file_name}' quality score was acceptable. No cleaning required.\"\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": useddata_guid,\n",
        "            \"attributes\": entity_attributes\n",
        "        })\n",
        "\n",
        "        inputs.append({\"guid\": useddata_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Notebook) ===\n",
        "    notebook_file = input_json.get(\"Notebook\", {}).get(\"file\", \"notebook.ipynb\")\n",
        "    notebook_guid = gen_guid()\n",
        "    notebook_format = clean_format(Path(notebook_file).suffix or \"ipynb\")\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"UsedData\",\n",
        "        \"guid\": notebook_guid,\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"Notebook: {notebook_file}\",\n",
        "            \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "            \"document\": notebook_file,\n",
        "            \"format\": notebook_format,\n",
        "            \"size\": safe_size(None, 85000),\n",
        "            \"qualifiedName\": generate_qualified_name(\"data\", f\"notebook-{notebook_file}\", project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    outputs.append({\"guid\": notebook_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Log) ===\n",
        "    log_data = input_json.get(\"Log\", {})\n",
        "    if log_data:\n",
        "        log_guid = gen_guid()\n",
        "        log_filename = log_data.get(\"filename\", \"output.log\")\n",
        "        log_size = safe_size(log_data.get(\"total_bytes\"))\n",
        "        log_format = clean_format(Path(log_filename).suffix or \"log\")\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": log_guid,\n",
        "            \"attributes\": {\n",
        "                \"name\": f\"Log: {log_filename}\",\n",
        "                \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "                \"document\": log_filename,\n",
        "                \"format\": log_format,\n",
        "                \"size\": log_size,\n",
        "                \"qualifiedName\": generate_qualified_name(\"data\", f\"log-{log_filename}\", project_safe_name)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        outputs.append({\"guid\": log_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Graphs) ===\n",
        "    graphs = input_json.get(\"CodeLine\", {}).get(\"graphs\", [])\n",
        "    for i, graph in enumerate(graphs):\n",
        "        graph_guid = gen_guid()\n",
        "\n",
        "        graph_cleaned = graph.replace(\"Unknown\", \"no model\")\n",
        "\n",
        "        if \"-\" in graph_cleaned:\n",
        "            document_name = graph_cleaned.split(\"-\", 1)[1].strip().replace(\" \", \"_\") + \".png\"\n",
        "        else:\n",
        "            document_name = graph_cleaned.strip().replace(\" \", \"_\") + \".png\"\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": graph_guid,\n",
        "            \"attributes\": {\n",
        "                \"name\": f\"Chart: {graph_cleaned}\", \n",
        "                \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "                \"document\": document_name,\n",
        "                \"format\": \"png\",\n",
        "                \"size\": safe_size(None, 45000),\n",
        "                \"qualifiedName\": generate_qualified_name(\"data\", f\"chart-{i}-{document_name}\", project_safe_name)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        outputs.append({\"guid\": graph_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Models) ===\n",
        "    model_metadata = input_json.get(\"Model\", [])\n",
        "    models_list = input_json.get(\"CodeLine\", {}).get(\"models\", [])\n",
        "    models_source = model_metadata if model_metadata else models_list\n",
        "\n",
        "    for i, model_name in enumerate(models_source):\n",
        "        model_guid = gen_guid()\n",
        "        document_name = f\"{model_name}.pickle\"\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": model_guid,\n",
        "            \"attributes\": {\n",
        "                \"name\": f\"Model: {model_name}\",\n",
        "                \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "                \"document\": document_name,\n",
        "                \"format\": \"pickle\",\n",
        "                \"size\": safe_size(None, 1000000),\n",
        "                \"qualifiedName\": generate_qualified_name(\"data\", f\"model-{model_name}\", project_safe_name)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        outputs.append({\"guid\": model_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === ACTION ===\n",
        "    input_documents = [file.get(\"name\", f\"file_{i}\") for i, file in enumerate(file_items)]\n",
        "    input_data_string = f\"Analysis of {len(input_documents)} datasets\"\n",
        "    if input_documents:\n",
        "        input_data_string += f\": {', '.join(input_documents)}\"\n",
        "\n",
        "    # Add information about cleaned CSVs to the action description\n",
        "    cleaned_files = [file.get(\"name\") for file in file_items if file.get(\"quality_info\", {}).get(\"is_cleaned_version\")]\n",
        "    if cleaned_files:\n",
        "        input_data_string += f\". CSVs cleaned and enhanced: {', '.join(cleaned_files)}\"\n",
        "\n",
        "    action_description = f\"Notebook {notebook_file} v{iteration_number}\"\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"Action\",\n",
        "        \"guid\": guids[\"action\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"Action - Notebook - {project_title}\",\n",
        "            \"status\": \"Completed\",\n",
        "            \"inputData\": input_data_string,\n",
        "            \"outputData\": f\"Generated {len(outputs)} outputs including models and visualizations\",\n",
        "            \"iteration\": {\"guid\": guids[\"iteration\"], \"typeName\": \"Iteration\"},\n",
        "            \"madeBy\": [{\"guid\": guids[\"user\"], \"typeName\": \"User\"}],\n",
        "            \"qualifiedName\": generate_qualified_name(\"action\", action_description, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"inputs\": inputs,\n",
        "            \"outputs\": outputs\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === CONSENSUS ===\n",
        "    consensus_guid = gen_guid()\n",
        "    entities.append({\n",
        "        \"typeName\": \"Consensus\",\n",
        "        \"guid\": consensus_guid,\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"Validation - {project_title}\",\n",
        "            \"typeConsensus\": \"Individual Review\",\n",
        "            \"agreementLevel\": 100,\n",
        "            \"resolvedBy\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "            \"result\": \"approved\",\n",
        "            \"action\": {\"guid\": guids[\"action\"], \"typeName\": \"Action\"},\n",
        "            \"qualifiedName\": generate_qualified_name(\"consensus\", f\"validation-{project_title}\", project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    return {\"entities\": entities}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKfY1ewMg_xb"
      },
      "source": [
        "# Execution Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D89fvB-qKYgX"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# MAIN UNIFIED PIPELINE\n",
        "# =====================================================\n",
        "\n",
        "for ref in kernel_refs:\n",
        "    print(f\"\\n[INFO] ========== Processing kernel: {ref} ==========\")\n",
        "\n",
        "    all_datasets = []\n",
        "    all_logs = []\n",
        "    all_dataset_files = []\n",
        "    all_notebook_insights = []\n",
        "\n",
        "    name = safe_name(ref)\n",
        "    kernel_dir = os.path.join(base_dir, name)\n",
        "    os.makedirs(kernel_dir, exist_ok=True)\n",
        "\n",
        "    # Create unified directory structure\n",
        "    datasets_dir = Path(kernel_dir) / \"datasets\"\n",
        "    datasets_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    cleaned_dir = datasets_dir / \"cleaned\"\n",
        "    cleaned_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    reports_dir = Path(kernel_dir) / \"reports\"\n",
        "    reports_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    visualizations_dir = Path(kernel_dir) / \"visualizations\"\n",
        "    visualizations_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # 1. Download metadata\n",
        "        api.kernels_pull(kernel=ref, path=kernel_dir, metadata=True)\n",
        "        metadata_path = os.path.join(kernel_dir, \"kernel-metadata.json\")\n",
        "        if not os.path.exists(metadata_path):\n",
        "            print(f\"[WARN] No metadata found for {ref}\")\n",
        "            continue\n",
        "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        # 2. Datasets with reliability and quality analysis\n",
        "        dataset_sources = meta.get(\"dataset_sources\", [])\n",
        "        for ds_ref in dataset_sources:\n",
        "            try:\n",
        "                owner, slug = ds_ref.split(\"/\", 1)\n",
        "            except ValueError:\n",
        "                print(f\"[WARN] Invalid dataset reference: {ds_ref}\")\n",
        "                continue\n",
        "            results = api.dataset_list(search=slug, user=owner)\n",
        "            for ds in results:\n",
        "                if ds.ref == ds_ref:\n",
        "                    all_datasets.append({\n",
        "                        \"id_kernel\": ref,\n",
        "                        \"dataset_source\": ds_ref,\n",
        "                        \"title\": ds.title,\n",
        "                        \"subtitle\": ds.subtitle,\n",
        "                        \"creator\": ds.creator_name,\n",
        "                        \"license\": ds.license_name,\n",
        "                        \"downloads\": ds.download_count,\n",
        "                        \"votes\": ds.vote_count,\n",
        "                        \"description\": ds.description,\n",
        "                        \"url\": f\"https://www.kaggle.com/datasets/{ds_ref}\",\n",
        "                        \"is_private\": ds.is_private,\n",
        "                        \"is_featured\": ds.is_featured\n",
        "                    })\n",
        "\n",
        "                    # 3. Dataset files\n",
        "                    try:\n",
        "                        files = api.dataset_list_files(ds_ref).files\n",
        "                        for f in files:\n",
        "                            local_path = os.path.join(datasets_dir, f._name)\n",
        "                            all_dataset_files.append({\n",
        "                                \"id_kernel\": ref,\n",
        "                                \"dataset_source\": ds_ref,\n",
        "                                \"file_name\": f._name,\n",
        "                                \"creation_date\": f._creation_date,\n",
        "                                \"total_bytes\": f._total_bytes,\n",
        "                                \"local_path\": local_path\n",
        "                            })\n",
        "\n",
        "                        # Download the complete dataset\n",
        "                        print(f\"[INFO] Downloading dataset {ds_ref} into {datasets_dir}\")\n",
        "                        api.dataset_download_files(ds_ref, path=str(datasets_dir), unzip=True)\n",
        "\n",
        "                        # Run reliability assessment\n",
        "                        print(f\"[INFO] Running reliability assessment for {ds_ref}...\")\n",
        "                        try:\n",
        "                            reliability_eval = dataset_evaluation(ds_ref)\n",
        "                            reliability_path = reports_dir / f\"dataset_reliability_{slug}.json\"\n",
        "                            safe_json_dump(reliability_eval, reliability_path, indent=2, ensure_ascii=False)\n",
        "                            print(f\"[INFO] Reliability report saved: {reliability_path}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"[WARN] Could not generate reliability report for {ds_ref}: {e}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] Getting or downloading files from {ds_ref}: {e}\")\n",
        "\n",
        "        # 3. Data Quality Analysis\n",
        "        print(f\"[INFO] Running data quality analysis...\")\n",
        "        if datasets_dir.exists() and any(datasets_dir.glob('*.csv')):\n",
        "            # Analyze all CSV files in datasets directory\n",
        "            analysis_report = analyze_multiple_csvs(datasets_dir)\n",
        "\n",
        "            # Save datasets analysis\n",
        "            datasets_analysis_path = reports_dir / \"datasets_analysis.json\"\n",
        "            safe_json_dump(analysis_report, datasets_analysis_path, indent=2, ensure_ascii=False)\n",
        "            print(f\"[INFO] Datasets analysis saved: {datasets_analysis_path}\")\n",
        "\n",
        "            # Process each CSV file for quality and cleaning\n",
        "            updated_analysis = {\"files_analysis\": {}}\n",
        "\n",
        "            for filename, file_data in analysis_report[\"files_analysis\"].items():\n",
        "                if file_data.get('errors'):\n",
        "                    print(f\"[WARN] Skipping {filename} due to errors: {file_data['errors']}\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate quality scores\n",
        "                scores = calculate_global_score(file_data)\n",
        "                print(f\"[INFO] Quality scores for {filename}: {scores}\")\n",
        "\n",
        "                dataset_path = datasets_dir / filename\n",
        "\n",
        "                # Generate visualizations\n",
        "                print(f\"[INFO] Generating visualizations for {filename}...\")\n",
        "                plot_histograms(file_data, save_dir=visualizations_dir)\n",
        "\n",
        "                # Clean dataset if below threshold\n",
        "                if scores[\"global_score\"] < ALERT_THRESHOLD and dataset_path.exists():\n",
        "                    print(f\"[ALERT] {filename} ({scores['global_score']:.2f}) below threshold. Cleaning...\")\n",
        "                    clean_path = clean_dataset(dataset_path, cleaned_dir)\n",
        "\n",
        "                    # Re-analyze cleaned dataset\n",
        "                    new_dataset_info = extract_csv_metadata(clean_path)\n",
        "                    new_scores = calculate_global_score(new_dataset_info)\n",
        "                    print(f\"[INFO] New scores after cleaning: {new_scores}\")\n",
        "\n",
        "                    if new_scores[\"global_score\"] >= ALERT_THRESHOLD:\n",
        "                        print(f\"[SUCCESS] Cleaning improved {filename}\")\n",
        "                        plot_histograms(new_dataset_info, save_dir=visualizations_dir)\n",
        "\n",
        "                    updated_analysis[\"files_analysis\"][f\"clean_{filename}\"] = new_dataset_info\n",
        "                else:\n",
        "                    updated_analysis[\"files_analysis\"][filename] = file_data\n",
        "                    if scores[\"global_score\"] >= ALERT_THRESHOLD:\n",
        "                        print(f\"[SUCCESS] {filename} meets quality standards\")\n",
        "\n",
        "            # Save updated analysis\n",
        "            clean_analysis_path = reports_dir / \"datasets_analysis_clean.json\"\n",
        "            safe_json_dump(updated_analysis, clean_analysis_path, indent=2, ensure_ascii=False)\n",
        "            print(f\"[INFO] Clean analysis saved: {clean_analysis_path}\")\n",
        "\n",
        "        # 4. Logs\n",
        "        output_dir = os.path.join(kernel_dir, \"outputs\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        try:\n",
        "            log_result = analyze_kaggle_log(kernel_ref=ref, output_path=output_dir)\n",
        "            log_result[\"id_kernel\"] = ref\n",
        "            all_logs.append(log_result)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Log analysis failed for {ref}: {e}\")\n",
        "\n",
        "        # 5. Notebook Insights\n",
        "        notebook_file = next((f for f in os.listdir(kernel_dir) if f.endswith(\".ipynb\")), None)\n",
        "        if notebook_file:\n",
        "            nb_path = os.path.join(kernel_dir, notebook_file)\n",
        "            insights = extract_insights_from_notebook(nb_path, project_id=ref)\n",
        "            insights[\"id_kernel\"] = ref\n",
        "            all_notebook_insights.append(insights)\n",
        "\n",
        "            insights_path = os.path.join(kernel_dir, \"insights-notebook.json\")\n",
        "            safe_json_dump(insights, insights_path, indent=2, ensure_ascii=False)\n",
        "        else:\n",
        "            print(f\"[WARN] No notebook found for {ref}\")\n",
        "            notebook_file = f\"No notebook found for {ref}\"\n",
        "\n",
        "        df_datasets = pd.DataFrame(all_datasets)\n",
        "        df_files = pd.DataFrame(all_dataset_files)\n",
        "        # 6. Generate entities-kaggle.json and entities-bulk-atlas.json\n",
        "        metadata_path = os.path.join(kernel_dir, \"kernel-metadata.json\")\n",
        "        if os.path.exists(metadata_path):\n",
        "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            username = data.get(\"id\", \"\").split(\"/\")[0] if \"/\" in data.get(\"id\", \"\") else data.get(\"id\", \"\")\n",
        "            project_data = {k: v for k, v in data.items() if k in [\"title\", \"keywords\"]}\n",
        "\n",
        "            # Use cleaned versions when available, original when they meet quality standards\n",
        "            files = []\n",
        "            quality_files_used = []\n",
        "            files_to_use = {}  # Initialize here to ensure it's always available\n",
        "\n",
        "            # Load the clean analysis to determine which files were cleaned\n",
        "            clean_analysis_path = reports_dir / \"datasets_analysis_clean.json\"\n",
        "            original_analysis_path = reports_dir / \"datasets_analysis.json\"\n",
        "\n",
        "            if clean_analysis_path.exists():\n",
        "                with open(clean_analysis_path, 'r', encoding='utf-8') as f:\n",
        "                    clean_analysis = json.load(f)\n",
        "\n",
        "                # Process the clean analysis to determine best files to use\n",
        "                for filename, file_data in clean_analysis.get(\"files_analysis\", {}).items():\n",
        "                    if file_data.get('errors'):\n",
        "                        continue\n",
        "\n",
        "                    scores = calculate_global_score(file_data)\n",
        "                    file_path = file_data.get('file_path')\n",
        "\n",
        "                    if filename.startswith(\"clean_\"):\n",
        "                        # This is a cleaned file\n",
        "                        original_name = filename[6:]  # Remove \"clean_\" prefix\n",
        "                        files_to_use[original_name] = (file_path, True, scores[\"global_score\"])\n",
        "                        quality_files_used.append(f\"{original_name} (cleaned version - score: {scores['global_score']:.2f})\")\n",
        "                    else:\n",
        "                        # This is an original file that met standards\n",
        "                        if filename not in files_to_use:  # Don't override if we already have a cleaned version\n",
        "                            files_to_use[filename] = (file_path, False, scores[\"global_score\"])\n",
        "                            quality_files_used.append(f\"{filename} (original version - score: {scores['global_score']:.2f})\")\n",
        "\n",
        "            # If no clean analysis, fall back to original files\n",
        "            elif original_analysis_path.exists():\n",
        "                with open(original_analysis_path, 'r', encoding='utf-8') as f:\n",
        "                    original_analysis = json.load(f)\n",
        "\n",
        "                for filename, file_data in original_analysis.get(\"files_analysis\", {}).items():\n",
        "                    if not file_data.get('errors'):\n",
        "                        scores = calculate_global_score(file_data)\n",
        "                        files_to_use[filename] = (file_data.get('file_path'), False, scores[\"global_score\"])\n",
        "                        quality_files_used.append(f\"{filename} (original - score: {scores['global_score']:.2f})\")\n",
        "\n",
        "            # Convert to the expected format for RPCM entities\n",
        "            for original_filename, (file_path, is_clean, score) in files_to_use.items():\n",
        "                if file_path and Path(file_path).exists():\n",
        "                    file_stats = Path(file_path).stat()\n",
        "                    files.append({\n",
        "                        \"name\": original_filename,  # Always use original name for consistency\n",
        "                        \"path\": file_path,  # But path points to best quality version\n",
        "                        \"totalbytes\": file_stats.st_size,\n",
        "                        \"quality_info\": {\n",
        "                            \"is_cleaned_version\": is_clean,\n",
        "                            \"quality_score\": score,\n",
        "                            \"meets_threshold\": score >= ALERT_THRESHOLD\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "            # Log which files are being used for Atlas\n",
        "            if quality_files_used:\n",
        "                print(f\"[INFO] Files selected for Atlas entities:\")\n",
        "                for file_info in quality_files_used:\n",
        "                    print(f\"  • {file_info}\")\n",
        "            else:\n",
        "                print(f\"[INFO] No quality-analyzed files found, using fallback method\")\n",
        "\n",
        "            # Fallback for files from Kaggle API if no local analysis\n",
        "            if not files:\n",
        "                print(f\"[INFO] Using Kaggle API files as fallback\")\n",
        "                for _, row in df_files[df_files[\"id_kernel\"] == ref].head(20).iterrows():\n",
        "                    file_name = row.get(\"file_name\", \"\").strip()\n",
        "                    if file_name:\n",
        "                        files.append({\n",
        "                            \"name\": file_name,\n",
        "                            \"path\": row.get(\"local_path\", None),\n",
        "                            \"totalbytes\": row.get(\"total_bytes\", None),\n",
        "                            \"quality_info\": {\n",
        "                                \"is_cleaned_version\": False,\n",
        "                                \"quality_score\": None,\n",
        "                                \"meets_threshold\": None\n",
        "                            }\n",
        "                        })\n",
        "\n",
        "            # Datasets from this kernel\n",
        "            datasets_info = []\n",
        "            for _, row in df_datasets[df_datasets[\"id_kernel\"] == ref].iterrows():\n",
        "                clean_title = re.sub(r'[^\\w\\s\\-.,]', '', str(row.get(\"title\", \"\")))\n",
        "                clean_subtitle = re.sub(r'[^\\w\\s\\-.,]', '', str(row.get(\"subtitle\", \"\")))\n",
        "                full_title = f\"{clean_title} - {clean_subtitle}\".strip(\" -\")\n",
        "                datasets_info.append({\"title\": full_title})\n",
        "\n",
        "            # Logs from this kernel\n",
        "            log_models = []\n",
        "            log_data = next((log for log in all_logs if log[\"id_kernel\"] == ref), None)\n",
        "            log_info = {}\n",
        "            if log_data:\n",
        "                file_info = log_data.get(\"file_info\", {})\n",
        "                log_info = {\n",
        "                    \"filename\": file_info.get(\"filename\"),\n",
        "                    \"filepath\": file_info.get(\"filepath\"),\n",
        "                    \"total_bytes\": file_info.get(\"total_bytes\")\n",
        "                }\n",
        "                log_models = [m[\"model\"] for m in log_data.get(\"models\", []) if m.get(\"model\")]\n",
        "\n",
        "            # CodeLine\n",
        "            insights = all_notebook_insights[0] if all_notebook_insights else {}\n",
        "            code_line = {\n",
        "                \"graphs\": [\n",
        "                    f\"{g['name']} - {g.get('model', 'Unknown')} - {g['section']}\"\n",
        "                    for g in insights.get(\"graphs\", [])\n",
        "                ]\n",
        "            }\n",
        "            if not log_models:\n",
        "                code_line[\"models\"] = insights.get(\"models\", [])\n",
        "\n",
        "            new_json = {\n",
        "                \"Project\": project_data,\n",
        "                \"Owner\": {\"name\": username},\n",
        "                \"Notebook\": {\"file\": notebook_file},\n",
        "                \"File\": files,\n",
        "                \"DataSets\": datasets_info,\n",
        "                \"Log\": log_info,\n",
        "                \"Model\": log_models,\n",
        "                \"CodeLine\": code_line\n",
        "            }\n",
        "\n",
        "            # Save entities-kaggle.json inside the entities/ folder.\n",
        "            entities_dir = os.path.join(kernel_dir, \"entities\")\n",
        "            os.makedirs(entities_dir, exist_ok=True)\n",
        "            entities_kaggle_path = os.path.join(entities_dir, \"entities-kaggle.json\")\n",
        "            safe_json_dump(new_json, entities_kaggle_path, indent=4, ensure_ascii=False)\n",
        "\n",
        "            print(f\"[INFO] Entities-Kaggle JSON saved: {entities_kaggle_path}\")\n",
        "\n",
        "            # Generate entities-bulk-atlas.json at the same level as kernel-metadata.json\n",
        "            entities_bulk_atlas_path = os.path.join(kernel_dir, \"entities/entities-bulk-atlas.json\")\n",
        "            atlas_entities = build_atlas_bulk_from_json(new_json, iteration_number=1)\n",
        "            safe_json_dump(atlas_entities, entities_bulk_atlas_path, indent=4, ensure_ascii=False)\n",
        "\n",
        "            print(f\"[INFO] Entities-Bulk-Atlas JSON saved: {entities_bulk_atlas_path}\")\n",
        "\n",
        "            # Generate quality summary report\n",
        "            quality_summary_path = reports_dir / \"pipeline_execution_report.json\"\n",
        "            quality_summary = {\n",
        "                \"pipeline_execution\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"kernel_ref\": ref,\n",
        "                    \"total_files_processed\": len(files_to_use) if files_to_use else 0\n",
        "                },\n",
        "                \"file_quality_decisions\": [],\n",
        "                \"atlas_entities_summary\": {\n",
        "                    \"total_entities_created\": len(atlas_entities.get(\"entities\", [])),\n",
        "                    \"datasets_sent_to_atlas\": len(files),\n",
        "                    \"quality_enhanced_datasets\": sum(1 for f in files if f.get(\"quality_info\", {}).get(\"is_cleaned_version\")),\n",
        "                    \"quality_verified_datasets\": sum(1 for f in files if not f.get(\"quality_info\", {}).get(\"is_cleaned_version\") and f.get(\"quality_info\", {}).get(\"meets_threshold\"))\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Only process quality decisions if files_to_use has data\n",
        "            if files_to_use:\n",
        "                for original_filename, (file_path, is_clean, score) in files_to_use.items():\n",
        "                    quality_summary[\"file_quality_decisions\"].append({\n",
        "                        \"original_filename\": original_filename,\n",
        "                        \"version_sent_to_atlas\": \"cleaned\" if is_clean else \"original\",\n",
        "                        \"file_path_used\": file_path,\n",
        "                        \"quality_score\": round(score, 2) if score is not None else None,\n",
        "                        \"meets_threshold\": score >= ALERT_THRESHOLD if score is not None else None,\n",
        "                        \"reason\": f\"{'Cleaned version used due to low quality score' if is_clean else 'Original version used - meets quality standards'}\"\n",
        "                    })\n",
        "            else:\n",
        "                quality_summary[\"file_quality_decisions\"].append({\n",
        "                    \"note\": \"No quality analysis data available - used fallback method with Kaggle API files\"\n",
        "                })\n",
        "\n",
        "            safe_json_dump(quality_summary, quality_summary_path, indent=2, ensure_ascii=False)\n",
        "            print(f\"[INFO] Quality summary report saved: {quality_summary_path}\")\n",
        "\n",
        "        print(f\"[SUCCESS] ========== Completed processing {ref} ==========\")\n",
        "\n",
        "\n",
        "        # Print quality decisions summary - only if we have quality data\n",
        "        if files_to_use:\n",
        "            print(f\"\\n[INFO] Quality Decisions Summary:\")\n",
        "            print(f\"  • Total datasets: {len(files)}\")\n",
        "            print(f\"  • Enhanced (cleaned): {sum(1 for f in files if f.get('quality_info', {}).get('is_cleaned_version'))}\")\n",
        "            print(f\"  • Verified (original): {sum(1 for f in files if not f.get('quality_info', {}).get('is_cleaned_version') and f.get('quality_info', {}).get('meets_threshold'))}\")\n",
        "            print(f\"  • Details saved in: pipeline_execution_report.json\")\n",
        "        else:\n",
        "            print(f\"\\n[INFO] Used fallback method for file selection - no quality analysis data available\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to process {ref}: {e}\")\n",
        "\n",
        "print(f\"\\n[SUCCESS] All kernels processed successfully!\")\n",
        "print(f\"[INFO] Results saved in: {base_dir}/\")\n",
        "print(f\"[INFO] Each project has the unified directory structure with:\")\n",
        "print(f\"  • Original datasets in /datasets/\")\n",
        "print(f\"  • Cleaned datasets in /datasets/cleaned/\")\n",
        "print(f\"  • Quality reports in /reports/\")\n",
        "print(f\"  • Execution logs in /outputs/\")\n",
        "print(f\"  • Visualizations in /visualizations/\")\n",
        "print(f\"  • RPCM entities in /entities/\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.10.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
