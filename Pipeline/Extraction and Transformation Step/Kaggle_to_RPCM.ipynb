{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "016FekXCJ9gH"
      },
      "source": [
        "# Pipeline for transforming metadata from Kaggle to Research Processes Curation Metamodel (RPCM) implementation\n",
        "\n",
        "This notebook details the extraction and transformation process of metadata from the Kaggle platform using its official API to access dataset information and associated attributes.\n",
        "\n",
        "The pipeline begins by connecting to the Kaggle API, where metadata such as dataset names, descriptions, schemas, tags, and other fields from Kaggle’s metamodel are programmatically extracted. This data is then processed and transformed to generate a bulk set of structured entities compatible with the RPCM metadata model implemented in ATLAS.\n",
        "\n",
        "\n",
        "This pipeline automates the integration of metadata between Kaggle and RPCM, enabling centralized management, traceability, and data enrichment within data governance environments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ0SY-1kwE6Y"
      },
      "source": [
        "## Kaggle API Settings and Libraries\n",
        "\n",
        "To use the Kaggle API, it is necessary to create an account on Kaggle and generate an API key. This key will allow us to authenticate and make requests to the Kaggle API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BASDmOC6wpCy",
        "outputId": "b07c4e7e-ce43-49bc-ecb1-3ced5b594a5a"
      },
      "outputs": [],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_neQIBsXzzeg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import chardet\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "import nbformat\n",
        "from pathlib import Path\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-La_qgnA0G3x"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Set your Kaggle credentials:\n",
        "os.environ['KAGGLE_USERNAME'] = \"abnerasteteh\"\n",
        "os.environ['KAGGLE_KEY'] = \"5dec29fba975d4119a37855b3653d27d\"\n",
        "\n",
        "# Authentication\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOfGPvfWwILN"
      },
      "source": [
        "## Download Kaggle Projects\n",
        "\n",
        "The Kaggle API was used to select and download a list of projects, chosen based on the number of user votes on the platform. These projects were manually reviewed to ensure variety, relevant content for information extraction, and that they were developed in Python. These example projects were selected because they include a diverse set of associated datasets and offer rich content within the notebook.\n",
        "\n",
        "Using the Kaggle API, the following items were downloaded or retrieved:\n",
        "\n",
        "* The complete project notebook in .ipynb format.\n",
        "* The project metadata (kernel-metadata.json).\n",
        "* The files associated with the datasets used by the project.\n",
        "* Execution logs of the kernel, when available.\n",
        "* Insights derived from the analysis and processing of the notebook.\n",
        "\n",
        "Additionally, data sources used in each project were extracted from the downloaded metadata, creating a DataFrame for identification and further analysis.\n",
        "\n",
        "## Files and Data Generated\n",
        "\n",
        "Based on the information obtained (metadata, notebooks, datasets, and logs), structured JSON files were generated to facilitate analysis and integration with external tools:\n",
        "\n",
        "* entities-kaggle.json: contains an enriched structure with information about the project, owner, datasets, files, logs, and details extracted from the notebook.\n",
        "\n",
        "* entities-bulk-atlas.json: a file prepared for consumption by the Atlas API, enabling bulk processing of entities related to the project.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uiML1pf_MWV"
      },
      "outputs": [],
      "source": [
        "# List of Kaggle projects to which the pipeline will be applied,\n",
        "# The code can run multiple projects at the same time, but the Kaggle API may stop because it has a call limit.\n",
        "kernel_refs = [\n",
        "    \"joelknapp/student-performance-analysis/\",\n",
        "    #\"mfaaris/content-based-and-tensorflow-recommender-system/\",\n",
        "    #\"jph84562/the-ugly-truth-of-people-decisions-in-speed-dating\",\n",
        "    #\"aremoto/retail-sales-forecast/\",\n",
        "    #\"nareshbhat/eda-classification-ensemble-92-accuracy\",\n",
        "    #\"fedi1996/boston-crime-analysis-with-plotly\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcCAea_V_N4g"
      },
      "outputs": [],
      "source": [
        "# Creation of the base directory and complementary functions\n",
        "base_dir = \"kaggle_notebooks\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "def safe_name(ref):\n",
        "    return re.sub(r'[^\\w\\-]', '_', ref)\n",
        "\n",
        "def gen_guid():\n",
        "    return f\"-{uuid.uuid4().int % 1000000000}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xTbClnr5Mjb"
      },
      "source": [
        "## Automated Extraction of Kaggle Kernel Outputs - Logs\n",
        "\n",
        "\n",
        "Here we implements a pipeline that, given a Kaggle kernel, automatically downloads its output files using the Kaggle CLI, then locates and loads the .log file in JSON format to process its content. It uses automatic detection of the file’s encoding to ensure correct reading and specifically extracts the text generated in the standard output stream (stdout). Through regular expressions and line-by-line analysis, it extracts relevant information such as the dataset structure (number of rows, columns, data types, and memory usage), details of trained models (name, accuracy, confusion matrix, and detailed class-level classification metrics), as well as the kernel’s execution times. Additionally, it collects metadata from the log file such as its size, creation date, and number of lines. Finally, it consolidates all this information into a structured JSON called log_analysis.json, thereby facilitating the integration of these data with other systems or enabling further automated and reproducible analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxQ_GXWFV9pu"
      },
      "outputs": [],
      "source": [
        "def download_kaggle_output(kernel_ref: str, download_path: str):\n",
        "    print(f\"[INFO] Downloading outputs from: {kernel_ref}\")\n",
        "    subprocess.run([\n",
        "        \"kaggle\", \"kernels\", \"output\", kernel_ref,\n",
        "        \"-p\", download_path\n",
        "    ], check=True)\n",
        "    print(f\"[INFO] Download completed at: {download_path}\")\n",
        "\n",
        "# ---------- LOAD AND PROCESS JSON LOG ----------\n",
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return chardet.detect(f.read(100000))['encoding']\n",
        "\n",
        "def load_log_json(file_path):\n",
        "    encoding = detect_encoding(file_path)\n",
        "    with open(file_path, 'r', encoding=encoding) as f:\n",
        "        return json.load(f), encoding\n",
        "\n",
        "def extract_text_from_stdout(log_data):\n",
        "    return ''.join(entry['data'] for entry in log_data if entry['stream_name'] == 'stdout')\n",
        "\n",
        "# ---------- INFORMATION PARSING ----------\n",
        "def extract_dataset_info(text):\n",
        "    info = {}\n",
        "    match = re.search(r\"RangeIndex: (\\d+)\", text)\n",
        "    if match:\n",
        "        info[\"rows\"] = int(match.group(1))\n",
        "    match = re.search(r\"Data columns \\(total (\\d+) columns\\):\", text)\n",
        "    if match:\n",
        "        info[\"columns\"] = int(match.group(1))\n",
        "    dtypes = re.findall(r\"\\d+\\s+(\\w+)\\s+\\d+ non-null\\s+(\\w+)\", text)\n",
        "    info[\"dtypes\"] = {col: dtype for col, dtype in dtypes}\n",
        "    match = re.search(r\"memory usage:\\s+([^\\n]+)\", text)\n",
        "    if match:\n",
        "        info[\"memory_usage\"] = match.group(1)\n",
        "    return info\n",
        "\n",
        "def extract_models_info(text):\n",
        "    lines = text.splitlines()\n",
        "    models = []\n",
        "    current = {\n",
        "        \"model\": None,\n",
        "        \"accuracy\": None,\n",
        "        \"conf_matrix\": [],\n",
        "        \"class_metrics\": {},\n",
        "        \"avg_metrics\": {},\n",
        "    }\n",
        "\n",
        "    state = None\n",
        "    pending_matrix = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # --- Confusion Matrix ---\n",
        "        if line.lower().startswith(\"confussion matrix\"):\n",
        "            state = \"conf_matrix\"\n",
        "            pending_matrix = []\n",
        "            continue\n",
        "        elif state == \"conf_matrix\" and \"[\" in line:\n",
        "            numbers = list(map(int, re.findall(r\"\\d+\", line)))\n",
        "            if len(numbers) == 2:\n",
        "                pending_matrix.append(numbers)\n",
        "            if len(pending_matrix) == 2:\n",
        "                current[\"conf_matrix\"] = pending_matrix\n",
        "                state = None\n",
        "            continue\n",
        "\n",
        "        # --- Accuracy ---\n",
        "        match = re.match(r\"Accuracy of (.*?):\\s*([\\d.]+)\", line)\n",
        "        if match:\n",
        "            current[\"model\"] = match.group(1).strip()\n",
        "            current[\"accuracy\"] = float(match.group(2))\n",
        "            continue\n",
        "\n",
        "        # --- Start of classification report ---\n",
        "        if re.match(r\"^precision\\s+recall\\s+f1-score\\s+support\", line):\n",
        "            state = \"report\"\n",
        "            continue\n",
        "\n",
        "        if state == \"report\":\n",
        "            parts = re.split(r\"\\s+\", line)\n",
        "            if len(parts) == 5:\n",
        "                label = parts[0]\n",
        "                try:\n",
        "                    metrics = {\n",
        "                        \"precision\": float(parts[1]),\n",
        "                        \"recall\": float(parts[2]),\n",
        "                        \"f1-score\": float(parts[3]),\n",
        "                        \"support\": int(parts[4]),\n",
        "                    }\n",
        "                    if label in (\"0\", \"1\"):\n",
        "                        current[\"class_metrics\"][label] = metrics\n",
        "                    elif label in (\"macro\", \"weighted\"):\n",
        "                        # Merge label + \" avg\"\n",
        "                        current[\"avg_metrics\"][label + \" avg\"] = metrics\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            elif \"accuracy\" in line.lower():\n",
        "                state = None\n",
        "                models.append(current)\n",
        "                current = {\n",
        "                    \"model\": None,\n",
        "                    \"accuracy\": None,\n",
        "                    \"conf_matrix\": [],\n",
        "                    \"class_metrics\": {},\n",
        "                    \"avg_metrics\": {},\n",
        "                }\n",
        "\n",
        "    if current[\"model\"] and current not in models:\n",
        "        models.append(current)\n",
        "\n",
        "    return models\n",
        "\n",
        "def extract_execution_time(log_data):\n",
        "    times = [entry[\"time\"] for entry in log_data if \"time\" in entry]\n",
        "    if not times:\n",
        "        return {}\n",
        "    return {\n",
        "        \"start\": times[0],\n",
        "        \"end\": times[-1],\n",
        "        \"duration_seconds\": round(times[-1] - times[0], 3)\n",
        "    }\n",
        "\n",
        "def extract_file_metadata(file_path, encoding):\n",
        "    stat = os.stat(file_path)\n",
        "    with open(file_path, 'r', encoding=encoding) as f:\n",
        "        num_lines = sum(1 for _ in f)\n",
        "    return {\n",
        "        \"filename\": os.path.basename(file_path),\n",
        "        \"filepath\": os.path.abspath(file_path),\n",
        "        \"created_at\": datetime.fromtimestamp(stat.st_ctime).isoformat(),\n",
        "        \"num_lines\": num_lines,\n",
        "        \"encoding\": encoding,\n",
        "        \"total_bytes\": stat.st_size\n",
        "    }\n",
        "\n",
        "def analyze_kaggle_log(kernel_ref: str, output_path: str):\n",
        "    download_kaggle_output(kernel_ref, output_path)\n",
        "\n",
        "    # Find the downloaded .log file\n",
        "    log_file = next((f for f in os.listdir(output_path) if f.endswith('.log')), None)\n",
        "    if not log_file:\n",
        "        raise FileNotFoundError(\"No .log file found in the download.\")\n",
        "    log_path = os.path.join(output_path, log_file)\n",
        "\n",
        "    log_data, encoding = load_log_json(log_path)\n",
        "    text = extract_text_from_stdout(log_data)\n",
        "\n",
        "    result = {\n",
        "        \"file_info\": extract_file_metadata(log_path, encoding),\n",
        "        \"dataset_info\": extract_dataset_info(text),\n",
        "        \"models\": extract_models_info(text),\n",
        "        \"execution_time\": extract_execution_time(log_data)\n",
        "    }\n",
        "\n",
        "    # Display and save result\n",
        "    #print(json.dumps(result, indent=2, ensure_ascii=False))\n",
        "    with open(os.path.join(output_path, \"log_analysis.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjFAWC3ckE_w"
      },
      "source": [
        "##  Extraction of Insights from Jupyter Notebooks\n",
        "\n",
        "This function parses a Jupyter notebook file to automatically extract key insights about its content. It reads the notebook in JSON format and scans through all its cells to identify and collect information such as the datasets being loaded (specifically from calls to pd.read_csv with paths pointing to input folders), machine learning models referenced (e.g., classifiers like XGBClassifier or SVC), and evaluation metrics used (such as accuracy, F1 score, confusion matrix, and classification reports). Additionally, it detects the presence of visual outputs like graphs by checking for .show() calls or embedded images in the notebook’s output cells. The function also tracks the notebook’s structural sections based on markdown headers, associating each extracted insight with its corresponding section title. All this information is aggregated into a structured dictionary, converting sets to lists for easy JSON serialization, thereby enabling automated analysis and summarization of notebook contents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvmudopukvqu"
      },
      "outputs": [],
      "source": [
        "def extract_insights_from_notebook(notebook_path, project_id=None):\n",
        "    with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "    insights = {\n",
        "        \"datasets\": set(),\n",
        "        \"models\": set(),\n",
        "        \"metrics\": set(),\n",
        "        \"graphs\": [],\n",
        "        \"sections\": []\n",
        "    }\n",
        "\n",
        "    current_section = \"Unknown\"\n",
        "    figure_count = 1\n",
        "    seen_graphs = set()\n",
        "\n",
        "    # List of models found per cell\n",
        "    model_by_cell = []\n",
        "\n",
        "    for idx, cell in enumerate(nb.cells):\n",
        "        if cell.cell_type == \"markdown\":\n",
        "            headers = re.findall(r'^#{1,6}\\s+(.*)', cell.source, re.MULTILINE)\n",
        "            if headers:\n",
        "                current_section = headers[0].strip()\n",
        "                insights[\"sections\"].append(current_section)\n",
        "\n",
        "        if cell.cell_type == \"code\":\n",
        "            code = cell.source\n",
        "\n",
        "            # === Datasets ===\n",
        "            csv_paths = re.findall(r\"pd\\.read_csv\\(['\\\"](.+?/input/[^/]+/[^'\\\"]+)['\\\"]\", code)\n",
        "            for path in csv_paths:\n",
        "                file_match = re.search(r\"/([^/]+\\.csv)\", path)\n",
        "                if file_match:\n",
        "                    insights[\"datasets\"].add(file_match.group(1))\n",
        "\n",
        "            # === Models ===\n",
        "            models_found = re.findall(r'\\b([A-Z][a-zA-Z]+Classifier|SVC|XGBClassifier)\\b', code)\n",
        "            if models_found:\n",
        "                insights[\"models\"].update(models_found)\n",
        "            model_by_cell.append(models_found)\n",
        "\n",
        "            # === Metrics ===\n",
        "            for m in [\"accuracy_score\", \"f1_score\", \"confusion_matrix\", \"classification_report\"]:\n",
        "                if m in code:\n",
        "                    insights[\"metrics\"].add(m)\n",
        "\n",
        "            # === Graphs associated with a model ===\n",
        "            has_show = \".show()\" in code\n",
        "            has_output = any(\n",
        "                out.get(\"output_type\") == \"display_data\" and \"image/png\" in out.get(\"data\", {})\n",
        "                for out in cell.get(\"outputs\", [])\n",
        "            )\n",
        "\n",
        "            if has_show or has_output:\n",
        "                # Attempt to associate with a model in the same cell\n",
        "                model_for_graph = models_found[0] if models_found else None\n",
        "\n",
        "                # If there is no model in the same cell, search in previous cells.\n",
        "                if not model_for_graph:\n",
        "                    for prev_models in reversed(model_by_cell[:-1]):\n",
        "                        if prev_models:\n",
        "                            model_for_graph = prev_models[0]\n",
        "                            break\n",
        "\n",
        "                # Avoid exact duplicates\n",
        "                graph_signature = (project_id, current_section, model_for_graph)\n",
        "                if graph_signature not in seen_graphs:\n",
        "                    insights[\"graphs\"].append({\n",
        "                        \"name\": f\"Figure {figure_count}\",\n",
        "                        \"section\": current_section,\n",
        "                        \"model\": model_for_graph if model_for_graph else \"Unknown\"\n",
        "                    })\n",
        "                    seen_graphs.add(graph_signature)\n",
        "                    figure_count += 1\n",
        "\n",
        "    # Convert sets to lists\n",
        "    insights[\"datasets\"] = list(insights[\"datasets\"])\n",
        "    insights[\"models\"] = list(insights[\"models\"])\n",
        "    insights[\"metrics\"] = list(insights[\"metrics\"])\n",
        "\n",
        "    return insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G524qwPW1jWs"
      },
      "source": [
        "## Building RPCM entities from Kaggle metadata\n",
        "\n",
        "This function is designed to transform a structured input dictionary (extracted metadata from Kaggle) into a collection of entities formatted to be ingested into the ATLAS metadata repository.  \n",
        "\n",
        "The output is a **list of dictionaries**, each representing an ATLAS entity with assigned types, GUIDs, and attributes—capturing a rich hierarchy and provenance of the data science workflow.  \n",
        "\n",
        "This bulk of entities is then ready to be ingested by ATLAS, enabling tracking and governance of data science workflows with provenance and lineage.\n",
        "\n",
        "\n",
        "\n",
        "### Step-by-step breakdown\n",
        "\n",
        "## Building RPCM entities from Kaggle metadata\n",
        "\n",
        "This function is designed to transform a structured input dictionary (extracted metadata from Kaggle) into a collection of entities formatted to be ingested into the ATLAS metadata repository.  \n",
        "\n",
        "The output is a **list of dictionaries**, each representing an ATLAS entity with assigned types, GUIDs, and attributes—capturing a rich hierarchy and provenance of the data science workflow.  \n",
        "\n",
        "This bulk of entities is then ready to be ingested by ATLAS, enabling tracking and governance of data science workflows with provenance and lineage.\n",
        "\n",
        "\n",
        "\n",
        "### Step-by-step breakdown\n",
        "\n",
        "1. Helper Functions & GUID Generation\n",
        "The function begins with robust helper functions for data validation:\n",
        "\n",
        "- generate_qualified_name(): Creates unique, Atlas-compatible qualified names with sanitized characters\n",
        "- safe_timestamp(): Generates valid timestamps in milliseconds, avoiding format issues\n",
        "- safe_size(): Converts size values to integers with fallback defaults, handles null/invalid values\n",
        "- clean_format(): Sanitizes file format strings, removes problematic characters like leading dots\n",
        "\n",
        "- Then generates unique identifiers (GUIDs) for each primary entity type.These GUIDs serve as immutable unique references linking entities and establishing relationships.\n",
        "\n",
        "\n",
        "2. User Entity\n",
        "\n",
        "- Constructed using the owner information from the input JSON.\n",
        "- If the owner's name is missing → defaults to \"anonymous\".\n",
        "- Assigned a hardcoded role: \"Kaggle Contributor\".\n",
        "- Uses sanitized qualified names for Atlas compatibility.\n",
        "\n",
        "3. Project Entity\n",
        "\n",
        "- Project title: defaults to \"Untitled Project\".\n",
        "- Keywords: extracted from project data, defaults to [].\n",
        "- createdBy: links to the user GUID.\n",
        "- Start date: uses safe_timestamp() for proper millisecond format.\n",
        "- References an Experiment entity through relationshipAttributes.\n",
        "\n",
        "4. Experiment Entity\n",
        "\n",
        "- Linked to the project with proper GUID references.\n",
        "- Name uses the project title directly (simplified naming).\n",
        "- References one Stage entity through relationshipAttributes.\n",
        "\n",
        "5. Stage Entity\n",
        "\n",
        "- Represents a specific phase in the experiment workflow.\n",
        "- Name: \"Stage\" (simplified).\n",
        "- Status: \"Completed\".\n",
        "- References one Iteration entity through relationshipAttributes.\n",
        "\n",
        "6. Iteration Entity\n",
        "\n",
        "- Name = \"Iteration {iteration_number}\" (default = 1).\n",
        "- References its parent stage by GUID.\n",
        "- Uses safe qualified name generation.\n",
        "\n",
        "7. Used Data Entities (Inputs) \n",
        "\n",
        "- Processes datasets and files with improved data handling:\n",
        "\n",
        "  - File items: From \"File\" array with safe size conversion and format cleaning\n",
        "  - DataSets: Additional dataset processing capability\n",
        "  - Creates a UsedData entity for each with: Safe size conversion using safe_size() with 50KB default. Format cleaning via clean_format()\n",
        "  - Descriptive names: \"Dataset: {filename}\"\n",
        "  - All collected into inputs list for Action relationship.\n",
        "\n",
        "8. Used Data Entity for Notebook - As Output\n",
        "\n",
        "- Notebook file treated as output UsedData (workflow result).\n",
        "- Enhanced attributes:\n",
        "\n",
        "  - Proper format extraction with clean_format()\n",
        "  - Default size of 85KB using safe_size()\n",
        "  - Descriptive name: \"Notebook: {filename}\"\n",
        "\n",
        "9. Used Data Entity for Log \n",
        "\n",
        "- Conditional processing: Only creates entity if log data exists\n",
        "- Robust field extraction:\n",
        "  - Filename with fallback to \"output.log\"\n",
        "  - Size conversion using safe_size()\n",
        "  - Format cleaning for proper Atlas ingestion\n",
        "- Added to outputs list.\n",
        "\n",
        "10. Used Data Entities for Graphs \n",
        "\n",
        "- Extracted from \"graphs\" key in \"CodeLine\".\n",
        "- Parses graph names, handles \"-\" separators\n",
        "- Sanitizes spaces to underscores\n",
        "- Auto-appends .png extension\n",
        "\n",
        "- Consistent attributes:\n",
        "\n",
        "  - Format: \"png\" (without leading dot)\n",
        "  - Default size: 45KB via safe_size()\n",
        "  - Clean qualified names\n",
        "\n",
        "\n",
        "11. Used Data Entities for Models \n",
        "\n",
        "- Flexible extraction: From \"Model\" array OR \"CodeLine.models\"\n",
        "- Document names: \"{model_name}.pickle\"\n",
        "- Format: \"pickle\"\n",
        "- Default size: 1MB for realistic model files\n",
        "- All linked as outputs.\n",
        "\n",
        "12. Action Entity \n",
        "\n",
        "- Represents execution with detailed input/output tracking:\n",
        "  - inputData: String description of analyzed datasets (not array)\n",
        "  - outputData: Summary of generated outputs count\n",
        "  - Name: \"Action - Notebook - {project_title}\"\n",
        "  - madeBy as array with user reference\n",
        "- References inputs/outputs through relationshipAttributes.\n",
        "\n",
        "13. Workgroup Entity\n",
        "\n",
        "- Name: \"Workgroup - {project_title}\"\n",
        "- Description: \"Workgroup - {project_title}\" (as requested)\n",
        "- Users: Array containing the project creator/owner\n",
        "- Experiment: Links to the experiment entity via GUID reference\n",
        "\n",
        "14. Consensus Entity\n",
        "- typeConsensus: \"Individual Review\"\n",
        "- agreementLevel: 95 (numeric)\n",
        "- result: \"approved\" (lowercase for schema compliance)\n",
        "- Proper GUID linking to action and user entities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCfidviNcXdT"
      },
      "outputs": [],
      "source": [
        "def build_atlas_bulk_from_json(input_json: dict, iteration_number=1):\n",
        "    entities = []\n",
        "\n",
        "    # === HELPER FUNCTIONS ===\n",
        "    def generate_qualified_name(entity_type, name, project_name):\n",
        "        \"\"\"Generate a unique and valid qualifiedName\"\"\"\n",
        "        safe_name = str(name).lower().replace(\" \", \"-\").replace(\"_\", \"-\")[:50]\n",
        "        safe_project = str(project_name).replace(\" \", \"\").replace(\"_\", \"\")[:20]\n",
        "        return f\"{safe_name}@{safe_project}\"\n",
        "\n",
        "    def safe_timestamp():\n",
        "        \"\"\"Generates a valid timestamp in milliseconds\"\"\"\n",
        "        return int(datetime.today().timestamp() * 1000)\n",
        "\n",
        "    def safe_size(size_value, default=50000):\n",
        "        \"\"\"Convert size to valid integer, avoid None/null\"\"\"\n",
        "        if size_value is None or size_value == \"null\":\n",
        "            return default\n",
        "        try:\n",
        "            return int(size_value) if size_value else default\n",
        "        except (ValueError, TypeError):\n",
        "            return default\n",
        "\n",
        "    def clean_format(file_format):\n",
        "        \"\"\"Clean file format (no leading dots)\"\"\"\n",
        "        if not file_format:\n",
        "            return \"unknown\"\n",
        "        return str(file_format).lstrip(\".\")\n",
        "\n",
        "    # === GENERATE GUIDS ===\n",
        "    guids = {\n",
        "        \"user\": gen_guid(),\n",
        "        \"project\": gen_guid(),\n",
        "        \"experiment\": gen_guid(),\n",
        "        \"workgroup\": gen_guid(),  \n",
        "        \"stage\": gen_guid(),\n",
        "        \"iteration\": gen_guid(),\n",
        "        \"action\": gen_guid()\n",
        "    }\n",
        "\n",
        "    # === EXTRACT PROJECT INFO ===\n",
        "    project_data = input_json.get(\"Project\", {})\n",
        "    project_title = project_data.get(\"title\", \"Untitled Project\")\n",
        "    project_safe_name = project_title.replace(\" \", \"\").replace(\"-\", \"\")\n",
        "\n",
        "    # === USER ===\n",
        "    owner = input_json.get(\"Owner\", {})\n",
        "    user_name = owner.get(\"name\", \"anonymous\")\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"User\",\n",
        "        \"guid\": guids[\"user\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": user_name,\n",
        "            \"role\": \"Kaggle Contributor\",\n",
        "            \"qualifiedName\": generate_qualified_name(\"user\", user_name, project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === PROJECT ===\n",
        "    project_keywords = project_data.get(\"keywords\", [])\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"Project\",\n",
        "        \"guid\": guids[\"project\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": project_title,\n",
        "            \"keywords\": project_keywords,\n",
        "            \"createdBy\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "            \"startDate\": safe_timestamp(),\n",
        "            \"qualifiedName\": generate_qualified_name(\"project\", project_title, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"experiments\": [{\"guid\": guids[\"experiment\"], \"typeName\": \"Experiment\"}]\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === WORKGROUP === \n",
        "    workgroup_name = f\"Workgroup - {project_title}\"\n",
        "    workgroup_description = f\"Workgroup - {project_title}\"\n",
        "    \n",
        "    entities.append({\n",
        "        \"typeName\": \"Workgroup\",\n",
        "        \"guid\": guids[\"workgroup\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": workgroup_name,\n",
        "            \"description\": workgroup_description,\n",
        "            \"experiment\": {\"guid\": guids[\"experiment\"], \"typeName\": \"Experiment\"},\n",
        "            \"users\": [{\"guid\": guids[\"user\"], \"typeName\": \"User\"}],\n",
        "            \"qualifiedName\": generate_qualified_name(\"workgroup\", workgroup_name, project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === EXPERIMENT ===\n",
        "    experiment_title = f\"{project_title} - Experiment\"\n",
        "    entities.append({\n",
        "        \"typeName\": \"Experiment\",\n",
        "        \"guid\": guids[\"experiment\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"{project_title}\",\n",
        "            \"project\": {\"guid\": guids[\"project\"], \"typeName\": \"Project\"},\n",
        "            \"qualifiedName\": generate_qualified_name(\"experiment\", experiment_title, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"stages\": [{\"guid\": guids[\"stage\"], \"typeName\": \"Stage\"}],\n",
        "            \"workgroup\": [{\"guid\": guids[\"workgroup\"], \"typeName\": \"Workgroup\"}] \n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === STAGE ===\n",
        "    stage_name = \"Stage\"\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"Stage\",\n",
        "        \"guid\": guids[\"stage\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": stage_name,\n",
        "            \"experiment\": {\"guid\": guids[\"experiment\"], \"typeName\": \"Experiment\"},\n",
        "            \"status\": \"Completed\",\n",
        "            \"qualifiedName\": generate_qualified_name(\"stage\", stage_name, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"iterations\": [{\"guid\": guids[\"iteration\"], \"typeName\": \"Iteration\"}]\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === ITERATION ===\n",
        "    iteration_name = f\"Iteration {iteration_number}\"\n",
        "    entities.append({\n",
        "        \"typeName\": \"Iteration\",\n",
        "        \"guid\": guids[\"iteration\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": iteration_name,\n",
        "            \"stage\": {\"guid\": guids[\"stage\"], \"typeName\": \"Stage\"},\n",
        "            \"qualifiedName\": generate_qualified_name(\"iteration\", iteration_name, project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === COLLECT INPUTS/OUTPUTS ===\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "\n",
        "    # === USED DATA (from File + DataSets) ===\n",
        "    file_items = input_json.get(\"File\", [])\n",
        "    datasets = input_json.get(\"DataSets\", [])\n",
        "\n",
        "    for i, file in enumerate(file_items):\n",
        "        useddata_guid = gen_guid()\n",
        "\n",
        "        file_name = file.get(\"name\", f\"file_{i}\")\n",
        "        file_size = safe_size(file.get(\"totalbytes\"))\n",
        "        file_format = clean_format(Path(file_name).suffix or \"csv\")\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": useddata_guid,\n",
        "            \"attributes\": {\n",
        "                \"name\": f\"Dataset {i+1}: {file_name}\",\n",
        "                \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "                \"document\": file_name,\n",
        "                \"format\": file_format,\n",
        "                \"size\": file_size,\n",
        "                \"qualifiedName\": generate_qualified_name(\"data\", f\"dataset-{i}-{file_name}\", project_safe_name)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        inputs.append({\"guid\": useddata_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Notebook) ===\n",
        "    notebook_file = input_json.get(\"Notebook\", {}).get(\"file\", \"notebook.ipynb\")\n",
        "    notebook_guid = gen_guid()\n",
        "    notebook_format = clean_format(Path(notebook_file).suffix or \"ipynb\")\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"UsedData\",\n",
        "        \"guid\": notebook_guid,\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"Notebook: {notebook_file}\",\n",
        "            \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "            \"document\": notebook_file,\n",
        "            \"format\": notebook_format,\n",
        "            \"size\": safe_size(None, 85000),\n",
        "            \"qualifiedName\": generate_qualified_name(\"data\", f\"notebook-{notebook_file}\", project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    outputs.append({\"guid\": notebook_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Log) ===\n",
        "    log_data = input_json.get(\"Log\", {})\n",
        "    if log_data:\n",
        "        log_guid = gen_guid()\n",
        "        log_filename = log_data.get(\"filename\", \"output.log\")\n",
        "        log_size = safe_size(log_data.get(\"total_bytes\"))\n",
        "        log_format = clean_format(Path(log_filename).suffix or \"log\")\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": log_guid,\n",
        "            \"attributes\": {\n",
        "                \"name\": f\"Log: {log_filename}\",\n",
        "                \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "                \"document\": log_filename,\n",
        "                \"format\": log_format,\n",
        "                \"size\": log_size,\n",
        "                \"qualifiedName\": generate_qualified_name(\"data\", f\"log-{log_filename}\", project_safe_name)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        outputs.append({\"guid\": log_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Graphs) ===\n",
        "    graphs = input_json.get(\"CodeLine\", {}).get(\"graphs\", [])\n",
        "    for i, graph in enumerate(graphs):\n",
        "        graph_guid = gen_guid()\n",
        "\n",
        "        # Extract clean document name\n",
        "        if \"-\" in graph:\n",
        "            document_name = graph.split(\"-\", 1)[1].strip().replace(\" \", \"_\") + \".png\"\n",
        "        else:\n",
        "            document_name = graph.strip().replace(\" \", \"_\") + \".png\"\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": graph_guid,\n",
        "            \"attributes\": {\n",
        "                \"name\": f\"Chart: {graph}\",\n",
        "                \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "                \"document\": document_name,\n",
        "                \"format\": \"png\",  \n",
        "                \"size\": safe_size(None, 45000),\n",
        "                \"qualifiedName\": generate_qualified_name(\"data\", f\"chart-{i}-{document_name}\", project_safe_name)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        outputs.append({\"guid\": graph_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === USED DATA (Models) ===\n",
        "    model_metadata = input_json.get(\"Model\", [])\n",
        "    models_list = input_json.get(\"CodeLine\", {}).get(\"models\", [])\n",
        "    models_source = model_metadata if model_metadata else models_list\n",
        "\n",
        "    for i, model_name in enumerate(models_source):\n",
        "        model_guid = gen_guid()\n",
        "        document_name = f\"{model_name}.pickle\"\n",
        "\n",
        "        entities.append({\n",
        "            \"typeName\": \"UsedData\",\n",
        "            \"guid\": model_guid,\n",
        "            \"attributes\": {\n",
        "                \"name\": f\"Model: {model_name}\",\n",
        "                \"producer\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "                \"document\": document_name,\n",
        "                \"format\": \"pickle\",\n",
        "                \"size\": safe_size(None, 1000000),  # 1MB default\n",
        "                \"qualifiedName\": generate_qualified_name(\"data\", f\"model-{model_name}\", project_safe_name)\n",
        "            }\n",
        "        })\n",
        "\n",
        "        outputs.append({\"guid\": model_guid, \"typeName\": \"UsedData\"})\n",
        "\n",
        "    # === ACTION ===\n",
        "    input_documents = [file.get(\"name\", f\"file_{i}\") for i, file in enumerate(file_items)]\n",
        "    input_data_string = f\"Analysis of {len(input_documents)} datasets\"\n",
        "    if input_documents:\n",
        "        input_data_string += f\": {', '.join(input_documents)}\"\n",
        "\n",
        "    action_description = f\"Notebook {notebook_file} v{iteration_number}\"\n",
        "\n",
        "    entities.append({\n",
        "        \"typeName\": \"Action\",\n",
        "        \"guid\": guids[\"action\"],\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"Action - Notebook - {project_title}\",\n",
        "            \"status\": \"Completed\",\n",
        "            \"inputData\": input_data_string,  \n",
        "            \"outputData\": f\"Generated {len(outputs)} outputs including models and visualizations\",\n",
        "            \"iteration\": {\"guid\": guids[\"iteration\"], \"typeName\": \"Iteration\"},\n",
        "            \"madeBy\": [{\"guid\": guids[\"user\"], \"typeName\": \"User\"}],\n",
        "            \"qualifiedName\": generate_qualified_name(\"action\", action_description, project_safe_name)\n",
        "        },\n",
        "        \"relationshipAttributes\": {\n",
        "            \"inputs\": inputs,\n",
        "            \"outputs\": outputs\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # === CONSENSUS ===\n",
        "    consensus_guid = gen_guid()\n",
        "    entities.append({\n",
        "        \"typeName\": \"Consensus\",\n",
        "        \"guid\": consensus_guid,\n",
        "        \"attributes\": {\n",
        "            \"name\": f\"Validation - {project_title}\",\n",
        "            \"typeConsensus\": \"Individual Review\",\n",
        "            \"agreementLevel\": 100,\n",
        "            \"resolvedBy\": {\"guid\": guids[\"user\"], \"typeName\": \"User\"},\n",
        "            \"result\": \"approved\",  \n",
        "            \"action\": {\"guid\": guids[\"action\"], \"typeName\": \"Action\"},\n",
        "            \"qualifiedName\": generate_qualified_name(\"consensus\", f\"validation-{project_title}\", project_safe_name)\n",
        "        }\n",
        "    })\n",
        "\n",
        "    return {\"entities\": entities}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl0SP900AhPL"
      },
      "source": [
        "# Main Code for Extracting metadata from kaggle and creating RPCM Entities.\n",
        "\n",
        "Folder Structure and Function Integration Explanation\n",
        "\n",
        "This code manages downloading and analyzing Kaggle kernels, organizing each kernel into a specific folder within a base directory (base_dir). For each kernel, a folder with a safe name (kernel_dir) is created, where the following are stored:\n",
        "\n",
        "- The kernel metadata file (kernel-metadata.json) and the notebook file (.ipynb).\n",
        "- An outputs/ subfolder containing log analysis results (log_analysis.json).\n",
        "- An entities/ subfolder containing JSON files such as entities-kaggle.json and entities-bulk-atlas.json` that summarize and structure the processed information.\n",
        "\n",
        "Process Flow and Function Relationships\n",
        "\n",
        "- The analyze_kaggle_log function is called to analyze the downloaded log file, saving the results in the outputs/ folder.\n",
        "- The extract_insights_from_notebook function extracts relevant insights from the notebook .ipynb, with its results saved inside the kernel folder and incorporated into the entity JSON files.\n",
        "- Finally, the build_atlas_bulk_from_json function takes the consolidated information from entities-kaggle.json and generates a structured JSON for ATLAS, saved as entities/entities-bulk-atlas.json.\n",
        "\n",
        "This folder organization keeps the downloaded and generated data well-structured and enables automated integration of analyses, metadata, and results for each kernel in a reproducible and scalable manner.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bpKcVsUZwE4",
        "outputId": "37543f4c-e6eb-4bda-b4ad-571838725fe0"
      },
      "outputs": [],
      "source": [
        "for ref in kernel_refs:\n",
        "    print(f\"\\n[INFO] Processing kernel: {ref}\")\n",
        "\n",
        "    all_datasets = []\n",
        "    all_logs = []\n",
        "    all_dataset_files = []\n",
        "    all_notebook_insights = []\n",
        "\n",
        "    name = safe_name(ref)\n",
        "    kernel_dir = os.path.join(base_dir, name)\n",
        "    os.makedirs(kernel_dir, exist_ok=True)\n",
        "\n",
        "    slug = \"default\"\n",
        "    dataset_dir = os.path.join(kernel_dir, \"datasets\", slug)\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # 1. Download metadata\n",
        "        api.kernels_pull(kernel=ref, path=kernel_dir, metadata=True)\n",
        "        metadata_path = os.path.join(kernel_dir, \"kernel-metadata.json\")\n",
        "        if not os.path.exists(metadata_path):\n",
        "            print(f\"[WARN] No metadata found for {ref}\")\n",
        "            continue\n",
        "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        # 2. Datasets\n",
        "        dataset_sources = meta.get(\"dataset_sources\", [])\n",
        "        for ds_ref in dataset_sources:\n",
        "            try:\n",
        "                owner, slug = ds_ref.split(\"/\", 1)\n",
        "            except ValueError:\n",
        "                print(f\"[WARN] Invalid dataset reference: {ds_ref}\")\n",
        "                continue\n",
        "            results = api.dataset_list(search=slug, user=owner)\n",
        "            for ds in results:\n",
        "                if ds.ref == ds_ref:\n",
        "                    all_datasets.append({\n",
        "                        \"id_kernel\": ref,\n",
        "                        \"dataset_source\": ds_ref,\n",
        "                        \"title\": ds.title,\n",
        "                        \"subtitle\": ds.subtitle,\n",
        "                        \"creator\": ds.creator_name,\n",
        "                        \"license\": ds.license_name,\n",
        "                        \"downloads\": ds.download_count,\n",
        "                        \"votes\": ds.vote_count,\n",
        "                        \"description\": ds.description,\n",
        "                        \"url\": f\"https://www.kaggle.com/datasets/{ds_ref}\",\n",
        "                        \"is_private\": ds.is_private,\n",
        "                        \"is_featured\": ds.is_featured\n",
        "                    })\n",
        "\n",
        "\n",
        "                    # 3. Dataset files\n",
        "                    try:\n",
        "                        files = api.dataset_list_files(ds_ref).files\n",
        "                        for f in files:\n",
        "                            local_path = os.path.join(dataset_dir, f._name)\n",
        "                            all_dataset_files.append({\n",
        "                                \"id_kernel\": ref,\n",
        "                                \"dataset_source\": ds_ref,\n",
        "                                \"file_name\": f._name,\n",
        "                                \"creation_date\": f._creation_date,\n",
        "                                \"total_bytes\": f._total_bytes,\n",
        "                                \"local_path\": local_path\n",
        "                            })\n",
        "\n",
        "\n",
        "\n",
        "                        # Download the complete dataset in the datasets folder within the project\n",
        "                        dataset_dir = os.path.join(kernel_dir, \"datasets\", slug)\n",
        "                        os.makedirs(dataset_dir, exist_ok=True)\n",
        "                        print(f\"[INFO] Downloading dataset {ds_ref} into {dataset_dir}\")\n",
        "                        api.dataset_download_files(ds_ref, path=dataset_dir, unzip=True)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] Getting or downloading files from {ds_ref}: {e}\")\n",
        "\n",
        "\n",
        "        # 4. Logs\n",
        "        output_dir = os.path.join(kernel_dir, \"outputs\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        try:\n",
        "            analyze_kaggle_log(kernel_ref=ref, output_path=output_dir)\n",
        "            log_path = os.path.join(output_dir, \"log_analysis.json\")\n",
        "            if os.path.exists(log_path):\n",
        "                with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    log_data = json.load(f)\n",
        "                log_data[\"id_kernel\"] = ref\n",
        "                all_logs.append(log_data)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Log analysis failed for {ref}: {e}\")\n",
        "\n",
        "        # 5. Notebook Insights\n",
        "        notebook_file = next((f for f in os.listdir(kernel_dir) if f.endswith(\".ipynb\")), None)\n",
        "        if notebook_file:\n",
        "            nb_path = os.path.join(kernel_dir, notebook_file)\n",
        "            insights = extract_insights_from_notebook(nb_path, project_id=ref)\n",
        "            insights[\"id_kernel\"] = ref\n",
        "            all_notebook_insights.append(insights)\n",
        "\n",
        "            insights_path = os.path.join(kernel_dir, \"insights-notebook.json\")\n",
        "            with open(insights_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(insights, f, indent=2, ensure_ascii=False)\n",
        "        else:\n",
        "          print(f\"[WARN] No notebook found for {ref}\")\n",
        "          notebook_file = f\"No notebook found for {ref}\"\n",
        "\n",
        "        df_datasets = pd.DataFrame(all_datasets)\n",
        "        df_files = pd.DataFrame(all_dataset_files)\n",
        "\n",
        "        # 6. Generate entities-kaggle.json and entities-bulk-atlas.json\n",
        "        metadata_path = os.path.join(kernel_dir, \"kernel-metadata.json\")\n",
        "        if os.path.exists(metadata_path):\n",
        "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            username = data.get(\"id\", \"\").split(\"/\")[0] if \"/\" in data.get(\"id\", \"\") else data.get(\"id\", \"\")\n",
        "            project_data = {k: v for k, v in data.items() if k in [\"title\", \"keywords\"]}\n",
        "\n",
        "            # Files for this kernel\n",
        "            files = []\n",
        "            for _, row in df_files[df_files[\"id_kernel\"] == ref].head(20).iterrows():\n",
        "                file_name = row.get(\"file_name\", \"\").strip()\n",
        "                if file_name:\n",
        "                    files.append({\n",
        "                        \"name\": file_name,\n",
        "                        \"path\": row.get(\"local_path\", None),\n",
        "                        \"totalbytes\": row.get(\"total_bytes\", None)\n",
        "                    })\n",
        "\n",
        "            # Datasets from this kernel\n",
        "            datasets_info = []\n",
        "            for _, row in df_datasets[df_datasets[\"id_kernel\"] == ref].iterrows():\n",
        "                clean_title = re.sub(r'[^\\w\\s\\-.,]', '', str(row.get(\"title\", \"\")))\n",
        "                clean_subtitle = re.sub(r'[^\\w\\s\\-.,]', '', str(row.get(\"subtitle\", \"\")))\n",
        "                full_title = f\"{clean_title} - {clean_subtitle}\".strip(\" -\")\n",
        "                datasets_info.append({\"title\": full_title})\n",
        "\n",
        "            # Logs from this kernel\n",
        "            log_models = []\n",
        "            log_data = next((log for log in all_logs if log[\"id_kernel\"] == ref), None)\n",
        "            log_info = {}\n",
        "            if log_data:\n",
        "                file_info = log_data.get(\"file_info\", {})\n",
        "                log_info = {\n",
        "                    \"filename\": file_info.get(\"filename\"),\n",
        "                    \"filepath\": file_info.get(\"filepath\"),\n",
        "                    \"total_bytes\": file_info.get(\"total_bytes\")\n",
        "                }\n",
        "                log_models = [m[\"model\"] for m in log_data.get(\"models\", []) if \"model\" in m]\n",
        "\n",
        "            # CodeLine\n",
        "            code_line = {\n",
        "                \"graphs\": [\n",
        "                    f\"{g['name']} - {g.get('model', 'Unknown')} - {g['section']}\"\n",
        "                    for g in insights.get(\"graphs\", [])\n",
        "                ]\n",
        "            }\n",
        "            if not log_models:\n",
        "                code_line[\"models\"] = insights.get(\"models\", [])\n",
        "\n",
        "            new_json = {\n",
        "                \"Project\": project_data,\n",
        "                \"Owner\": {\"name\": username},\n",
        "                \"Notebook\": {\"file\": notebook_file},\n",
        "                \"File\": files,\n",
        "                \"DataSets\": datasets_info,\n",
        "                \"Log\": log_info,\n",
        "                \"Model\": log_models,\n",
        "                \"CodeLine\": code_line\n",
        "            }\n",
        "\n",
        "            # Save entities-kaggle.json inside the entities/ folder.\n",
        "            entities_dir = os.path.join(kernel_dir, \"entities\")\n",
        "            os.makedirs(entities_dir, exist_ok=True)\n",
        "            entities_kaggle_path = os.path.join(entities_dir, \"entities-kaggle.json\")\n",
        "            with open(entities_kaggle_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(new_json, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "            print(f\"[INFO] Entities-Kaggle JSON saved: {entities_kaggle_path}\")\n",
        "\n",
        "            # Generate entities-bulk-atlas.json at the same level as kernel-metadata.json\n",
        "            entities_bulk_atlas_path = os.path.join(kernel_dir, \"entities/entities-bulk-atlas.json\")\n",
        "            atlas_entities = build_atlas_bulk_from_json(new_json, iteration_number=1)\n",
        "            with open(entities_bulk_atlas_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(atlas_entities, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "            print(f\"[INFO] Entities-Bulk-Atlas JSON saved: {entities_bulk_atlas_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to process {ref}: {e}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
