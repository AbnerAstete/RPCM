# ğŸ“Š Data Quality Assessment & Cleaning Tool

A comprehensive Python tool for evaluating and improving dataset quality from Kaggle, with automatic reliability assessment, quality scoring, and intelligent data cleaning.

## ğŸ¯ What it does

Downloads datasets from Kaggle automatically
Evaluates dataset reliability (author reputation, community feedback, versions)
Analyzes data quality with weighted scoring system
Cleans datasets that fall below quality thresholds
Visualizes data distributions and outliers

## âš¡ Quick Start

Set your Kaggle credentials:

```python
os.environ['KAGGLE_USERNAME'] = "your_username"
os.environ['KAGGLE_KEY'] = "your_api_key"
```

## Change dataset and run
```python
dataset_name = "kaggle_user/dataset"
```

## ğŸ“ˆ Quality Scoring System

Weighted Components:

- Completeness (45%) - Missing data percentage.
- Uniqueness (25%) - Duplicate rows detection.
- Outliers (30%) - Extreme values in numeric columns.

Threshold: 75 points (datasets below this get auto-cleaned)

## ğŸ” Data Analysis Facets

Source Facet:

- Author reputation & activity metrics.
- Dataset popularity (downloads, votes, notebooks).
- Version history and traceability.
- License and documentation quality.

Data Facet Metrics:

- Missing values per column.
- Duplicate row detection.
- Statistical distributions.
- Outlier identification using IQR method.

Automated Cleaning:

- Missing values: Median imputation for numeric, row removal for categorical
- Duplicates: Complete removal
- Outliers: Quantile-based clipping (5th-95th percentiles)

## ğŸ“ Project Structure:
```
project/
â”œâ”€â”€ Data_Quality.py                    # Main analysis script
â”œâ”€â”€ datasets/                          # Downloaded datasets
â”‚   â”œâ”€â”€ original_file1.csv
â”‚   â”œâ”€â”€ original_file2.csv
â”‚   â””â”€â”€ cleaned/                       # Auto-generated cleaned datasets
â”‚       â”œâ”€â”€ clean_original_file1.csv
â”‚       â””â”€â”€ clean_original_file2.csv
â”œâ”€â”€ dataset_reliability_report.json    # Kaggle metadata & reputation
â”œâ”€â”€ datasets_analysis.json             # Original data quality metrics
â””â”€â”€ datasets_analysis_clean.json       # Post-cleaning analysis
```

## ğŸ•µï¸â€â™‚ï¸ Source Facet Report

The `dataset_reliability_report.json` provides a complete check to the Kaggle User:
### What's Inside

| Assessment Factor | Possible States | What It Evaluates |
|-------------------|-----------------|-------------------|
| **1. Author Info** | âœ… Available / âŒ Not available | Creator reputation, portfolio size, community impact |
| **2. Publication Date** | âœ… Available / âš ï¸ Missing | Temporal transparency, update history |
| **3. License** | âœ… Clear license / âš ï¸ Unknown license | Legal usage rights and restrictions |
| **4. External Source** | âœ… Described / âš ï¸ No description | Data origin and methodology documentation |
| **5. Traceability** | âœ… Version history / âš ï¸ No versions | Change tracking and data lineage |
| **6. Description** | âœ… Complete / âš ï¸ Basic | Title, subtitle, and documentation quality |
| **7. Community Feedback** | âœ… High engagement / âš ï¸ Low | Downloads, votes, and community adoption |


## ğŸ“Š Data Facet Analysis 

The `datasets_analysis.json` provides a complete health check to the CSV files:

### What's Inside
**File Basics**: Size, timestamps, and structure overview  
**Schema Details**: Column names, data types, and dimensions  
**Quality Metrics**: Missing values, duplicates, and completeness scores  
**Statistical Profiles**: Descriptive stats for numeric columns, top values for categorical  
**Sample Data**: Preview of actual values to verify content  

### Key Quality Indicators
- **Completeness Score** (0-100): Percentage of non-missing data
- **Duplicate Detection**: Identical rows that need removal  
- **Column Uniqueness**: How many distinct values per field
- **Data Type Validation**: Proper numeric vs categorical classification

 ğŸ“Š Visualizations:

- Histograms with mean/median lines and statistical annotations
- Boxplots with outlier detection and counts

# ğŸ› ï¸ Dependencies

- python
- kaggle 
- pandas
- numpy 
- matplotlib 
- seaborn 
- sklearn 
- pathlib

## ğŸ’¡ Tips

- Adjust WEIGHTS dictionary to prioritize different quality aspects
- Modify ALERT_THRESHOLD based on your quality requirements
- Customize cleaning parameters (lower_quantile, upper_quantile) for outlier sensitivity