# ğŸš€ Kaggle to RPCM Pipeline
An automated data science project extraction and metadata pipeline that transforms Kaggle notebooks into research-ready structured data compatible with Apache Atlas and RPCM frameworks.

## ğŸ¯ What It Does
Extracts complete ML project lifecycles from Kaggle notebooks including datasets, models, execution logs, and insights - then structures everything for enterprise data governance and research reproducibility.

## âš¡ Quick Start

Set your Kaggle credentials:
```python
os.environ['KAGGLE_USERNAME'] = "your_username"
os.environ['KAGGLE_KEY'] = "your_api_key"
```
Add your target kernels:
```python
kernel_refs = [
    "your-project/notebook-name/",
]
```

## ğŸ” Pipeline Components

1. Call Kaggle API:

- Downloads notebook metadata and source code.
- Pulls all referenced datasets with full schema.
- Captures execution logs.
- Extracts model training results and accuracy scores.

2. Extract Metadata:

- Code Analysis: Identifies models, datasets, visualizations, and metrics.
- Log Mining: Parses execution outputs for performance data and confusion matrices.
- Notebook Insights: Sections, models used, graphs generated, and data dependencies.
- Metadata Enrichment: File sizes, creation dates, encoding detection.

3. Entity Generation

- entities-kaggle.json: Clean project structure with all components
- entities-bulk-atlas.json: Apache Atlas-compatible bulk import format


## ğŸ“ Project Structure
```
kaggle_notebooks/
â”œâ”€â”€ project-name-analysis/              # Individual project folder
â”‚   â”œâ”€â”€ kernel-metadata.json           # Raw Kaggle metadata
â”‚   â”œâ”€â”€ notebook.ipynb                 # Downloaded notebook
â”‚   â”œâ”€â”€ datasets/                      # All referenced datasets
â”‚   â”‚   â””â”€â”€ default/
â”‚   â”‚       â”œâ”€â”€ dataset1.csv
â”‚   â”‚       â””â”€â”€ dataset2.csv
â”‚   â”œâ”€â”€ outputs/                       # Execution analysis
â”‚   â”‚   â”œâ”€â”€ output.log                # Raw execution log
â”‚   â”‚   â””â”€â”€ log_analysis.json         # Parsed performance metrics
â”‚   â”œâ”€â”€ insights-notebook.json        # Code analysis results
â”‚   â””â”€â”€ entities/                      # Generated metadata
â”‚       â”œâ”€â”€ entities-kaggle.json       # Project structure
â”‚       â””â”€â”€ entities-bulk-atlas.json   # Atlas import format

```

## ğŸ—ï¸ Research Components Mapped

- User: Kaggle contributor profile
- Project: Notebook title and keywords
- Experiment: Purpose of the project
- Stage: Execution Phase
- Iteration: Version or run instance
- Action: Creation of the Notebook
- UsedData: Datasets, models, charts, logs, notebook
- Consensus: Validation and results

## ğŸ§© Atlas Integration

- Bulk Import Ready: Direct upload to Apache Atlas
- Qualified Names: Unique entity identification
- Relationship Mapping: Input/output dependencies
- Metadata Standards: Enterprise governance compliance